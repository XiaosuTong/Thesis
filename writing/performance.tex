\chapter{Multi-factor designed experiment for performance of the 
Nonparametric-Regression Modeling}

In this chapter, we shift our gears to the analysis of the performance of the 
analysis method we proposed in previous chapter for a large spatial temporal data.
There are two groups of tunning parameters for our nonparametric analysis model.
One is the tunning parameters of statistical model, such as smoothing window, 
smoothing degree for both temporal fitting and spatial fitting, which we have
already demonstrated in \cite{} and \cite{} using cross-validation method. Another
group of tunning parameters are user-tunable MapReduce tunning parameters. Within
this chapter, we illustrate a full factorial experiment to study the affect of
these system tunning parameters to the modeling.

\section{Type of MapReudce Job}

There are three main steps in one MapReduce job: Map, Shuffle and Sort, Reduce.

Within our routine of analysis for spatial-temporal data, we categorize all 
necessary MapReduce job into two different groups. The first type of MapReduce
job is mainly focus on one of smoothing models, which is in either spatial or 
temporal dimension. We name it as $model$-$fitting$ job. For this type of job, it 
reads in a given type of division (either by time or by location) from HDFS, and 
then carries out one type of smoothing procedure, depends on what the division is,
within its Map step. There is no need for neither Reduce step nor shuffle and sort
step in this type of job since the division format is not changed. So the output 
of Map function is directly written onto HDFS.

Another type of job is focusing on switching from one division to another, which
we name as $swapping$ job. For this type of job, it reads in a given type of 
division from HDFS. And for each input key-value pair, the Map function generates
multiple intermediate key-value pairs with different keys. For instance, if by 
time division is read in, and observations of all locations in each month is one 
input key-value pair, then 7,738 intermediate key-value pairs is created with each
location ID as key in the Map function of $swap$-$to$-$location$ MapReduce job.
Then the output of Map function is first written to local disk of nodes 

\section{Modeling Routine}

The whole analysis routine consist of six MapReduce jobs, which are listing as
following in order:

\subsection{Reading in}

The first MapReduce job in the modeling routine is the Reading in job.
The raw text data files as mentioned in section~\ref{sec:Download}, which is 
sitting on HDFS, is first merged into one text file, and then duplicated by
row. Specifically, the time series of monthly maximum temperature of each station
is replicated based on original 48 years observations to include $2^{16}$ years 
which is $786,432$ monthly observations.
The total size of the text data file is 33 GB which includes monthly maximum
temperature observation over $786,432$ months at 7,738 different locations.

\begin{description}
  \item[Input] The duplicated raw text data file on HDFS is read in as input. Each
  row is read in as a key-value pair. The key is a unique row index, and value
  is the corresponding string in that row. 
  \item[Output] The value of each output key-value pair is a matrix with dimension
  7,738 by 2. Each row represents the maximum temperature and station id of one 
  location for a given month, and the key is the corresponding month index varies 
  from 1 to $786,432$. 
  \item[Map] Every block of the raw text data saved on HDFS is sent to one of 
  mapper with each row as one of input key-value pairs. By calling the R function
  \texttt{strsplit}, each filed of the row string is split apart. Since each
  row contains 12 monthly maximum temperature of a given year at a given location,
  we generate a intermediate key-value pair for each of them for all locations
  and all years. The key of 
  intermediate key-value pairs is the month index of corresponding row, and the 
  value is a vector with station id and maximum temperature.
  \item[Reduce] All intermediate key-value pairs that share the same month index
  are shuffled and sent to one Reducer. By calling R function \texttt{rbind}, 
  all intermediate values are row-binded into one matrix with dimension of 7,738
  by 2. Finally there are $786,432$ key-value pairs generated and saved on HDFS.
\end{description}

Notice that the computation complexity of each Mapper is very light, mainly just
\texttt{strsplit} and \texttt{rhcollect} function are called. On the other hand,
the size of output data of each Mapper is roughly close the input data. In other
words, a lot of intermediate key-value pairs, in term of size, are shuffled and
sorted and then sent to corresponding Reducers. For this type of job, the best 
performance can be obtained by assigning more memory from heap size of each JVM 
to use for shuffle and sorting, which can avoid multiple meaningless I/O to disk.  
On the reduce side, it is also very light with respect to computation. Only 
\texttt{rbind} function is called repeatedly. So the best performance is obtained 
when the intermediate data can reside entirely in memory of the Reducer JVM, 
which achieved by assigning more memory from the JVM of Reducer for the purpose
of holding as many as possible intermediate key-value pairs in memory. 

\subsection{Spatial Fitting of Original Observation}

By taking the subset by time, the second 
MapReduce job is applying spatial loess smoothing fitting to each month 
independently in parallel. Meanwhile, a RData file which contains meta-data such
as longitude, latitude, and elevation about all 7,738 stations are saved on HDFS.
These information is used for each spatial loess smoothing in the Map step. 

\begin{description}
  \item[Input] $786,432$ key-value pairs generated in the first reading in job.
  The key is the month index, and the value is a matrix with dimension of 7,738
  by 2. First column is location index, and another column is the corresponding 
  maximum temperature observation.
  \item[Output] The number of output key-value pairs is still $786,432$. The key
  is still the month index, but the value is a vector of spatial loess fitted
  value with length 7,738, one for each location. The order of fitted value in 
  each vector is kept as same based on the order of locations. So we can get rid
  of the location information from the output key-value pairs.
  \item[Map] The shared RData file containing all location information is first 
  copied and load into the global environment of each Mapper. The input matrix
  is merged with station information R object (a data.frame with 7,738 rows and
  \texttt{lon}, \texttt{lat}, and \texttt{elev} columns). Then \texttt{spaloess}
  function from package \texttt{Spaloess} is called to calculate the spatial
  loess smoothing value for each station at given month. After the fitting,
  all spatial information of each location is dropped. The key of each 
  intermediate key-value pair is month index, and value is vector of spatial 
  smoothing value in the same order of locations. 
  \item[Reduce] There is no Reduce need for this job. The intermediate key-value
  pairs are directly written to HDFS.
\end{description}

This particular MapReduce job is different than the first reading in job. Here
the Reduce step is not necessary. Moreover, the shuffle and sorting stage should
also be avoid to save unnecessary network traffic and multiple trips to the local 
disk, and the final results of Map are directly wrote to HDFS. Consequently, a 
lot of MapReduce tuning parameter is not needed to be 
considered in this job. The best performance of this job can be obtained by 
enlarging the memory assigned to each Mapper for the computation.

\subsection{Swapping from By-time Division to By-location Division}

After the spatial smoothing in each month, we have to switch the data from by 
month division to by location division in order to proceed the smoothing procedure
in time dimension. 

\begin{description}
\item[Input] $786,432$ key-value pairs. Key is the month index, and value is a 
vector with length of $7,738$. The order of the numeric values in each vector are 
the same in all $786,432$ key-value pairs.
\item[Output] $7,738$ output key-value pairs are generated. The key is changed
to be location index, and the corresponding value is matrix with dimension 
$786,432$ by 2. One column is the month index, and another column is the spatial
smoothed value.
\item[Map]An intermediate key-value pair is generated for each element of the
value of each input key-value pair. The intermediate key is the index of the 
element which is the location index, and the corresponding value is a vector with
two numbers, month index which is the input key, and the element itself.   
\item[Reduce]

\end{description}

\subsection{Temporal Fitting}

\subsection{Swapping from by location division to by time division}

\subsection{Spatial Fitting of Remainder}

readin, spatial-fit, swaptoLoc, stlplus-fit, swaptoTime, spatial-fit



\section{Experiment Design}

\subsection{Hadoop User Tuning Parameters}

\subsubsection{\texttt{mapreduce.task.io.sort.mb}}

This parameter controls the size of memory buffer, in megabytes, which is used 
to hold the map output in each Mapper. The value for it is integer varies from 1 
to 2047. The default value for this parameter is 100, which only allocates 100MB
of memory from heap size of JVM for saving map output. It is quite small in 
general. For jobs like reading in and swapping, it is definitely worth to increase
the value of this parameter to give more memory for holding output of each Mapper.

\subsubsection{\texttt{mapreduce.map.sort.spill.percent}}

This parameter works with \texttt{mapreduce.task.io.sort.mb} collectively to 
control the memory used for holding map output. Concretely, a circular memory 
buffer is allocated for each mapper to write intermediate output to. When the 
map output occupied \texttt{mapreduce.map.sort.spill.percent} percent of \\
\texttt{mapreduce.task.io.sort.mb}, the output is spilled to local disk of node
where Mapper is running. Meanwhile the Mapper is keeping writing output to this
circular memory buffer when spilling is proceeding. However if the memory buffer
is filled up during this time period, the Mapper is paused until the spill is
finished. Clearly, it is critical to set this parameter as well as the total 
amount of memory buffer to be high enough if there are numerous amount of output
generated by each Mapper. However, we do not want set them to be too large.
Because the memory buffer is still belongs to the Mapper JVM heap size, which is
decided by \texttt{mapreduce.map.java.opts}. Reserving too much memory from the
total heap size of JVM will leave limited memory usage for other processes sponsed
by the JVM, and indeed will force JVM to involve more garbage clean, which in 
turn will hurt the job performance.    

\subsubsection{\texttt{mapreduce.task.io.sort.factor}}

Every time when the contents in the memory buffer specified by \\
\texttt{mapreduce.task.io.sort.mb} reaches the spill percent threshold, a new
spill file is generated. So there may be multiple output files generated after 
the Map is finished. All of these spill files are merged into one sorted and
partitioned file. This is done in in rounds, and the number of files merged in 
each round is controlled by \texttt{mapreduce.task.io.sort.factor}. The default 
value is 10.


\subsubsection{\texttt{mapreduce.job.reduce.slowstart.completedmaps}}

The parameter controls when Reducers should be launched. Specifically, it specifies
the fraction of the number of Mappers which should be complete before Reducers
are scheduled for the job. Under MapReduce2 (YARN) \cite{YARN} framework, this parameter
becomes critical if the Map of MapReduce job is time consuming. Setting this 
parameter to be a low value as default, which is 0.05, can start the Reducers
doing nothing but waiting for the output from Mappers. However this waist several
containers assigned for Reducers without doing anything instead of assigning them
to the Mappers under pending statues.   

\subsubsection{\texttt{mapreduce.reduce.shuffle.parallelcopies}}

After the map step is finished, the intermediate key-value pairs are partitioned
and sitting on the local disk of the node where Mapper ran. Then each Reducer 
the corresponding partition from all Mapper outputs through the network. And
actually each Reducer evokes a number of copy threads, which controlled by this
parameter, to fetch the intermediate output of Mapper in parallel. This parameter
should be set wisely. Too large value of copy threads will waste for the CPU,
but too small number of copy threads will slow down the copy step of Reducer.
The default value is 5.

\subsubsection{\texttt{mapreduce.reduce.shuffle.input.buffer.percent}}

During the copying stage, the output of Map is copied to part of the total heap 
space of Reducer's JVM. This parameter controls how large the proportion is.
The default value of this parameter is 0.7. Suppose the total heap size of the
JVM of Reducer is 4GB, then 2.8 GB of JVM's memory is used for holding the output
copied from the local disk of nodes ran Mappers.

\subsubsection{\texttt{mapreduce.reduce.shuffle.merge.percent}}

Similar with the Mapper, there is also a spilling mechanism to avoid memory 
overflow. Specifically, the Map output is consistently copied to the proportion 
of heap space specified by \texttt{mapreduce.reduce.shuffle.input.buffer.percent}.
Once the contents in the memory buffer reaches threshold controlled by two 
parameters collectively, the intermediate outputs or the inputs to Reduce is 
spilled to local disk of the node which runs the Reduce. One of the two parameters 
is \\
\texttt{mapreduce.reduce.shuffle.merge.percent} which controls the threshold in 
term of percent of buffer size. If the size of buffer size is occupied over this
proportion, spilling to disk will be triggered. By default, this is set to be 
0.66. Suppose the heap size of Reduce JVM is 4 GB as well, the memory buffer for
coping is 2.8 GB by setting \texttt{mapreduce.reduce.shuffle.input.buffer.percent}
to be 0.7. Then the size threshold is 1.85 GB. Another parameter controls the
threshold is the \texttt{mapreduce.reduce.merge.inmem.threshold} which is discussed
in the following paragraph.

\subsubsection{\texttt{mapreduce.reduce.merge.inmem.threshold}}

Parameter \texttt{mapreduce.reduce.shuffle.merge.percent} controls the memory
buffer threshold in term of the size of contents. Meanwhile the parameter \\
\texttt{mapreduce.reduce.merge.inmem.threshold} controls the contents in memory
buffer in term of counts. So the intermediate output held in the memory buffer
cannot be too many or too large, otherwise they will be spilled to local disk.
The default is 1,000. By setting this parameter to be 0 will hand over the control
of spilling behaviors fully to the \texttt{mapreduce.reduce.shuffle.merge.percent}. 
Notice that the spilled files on local disk are sorted by key. Once all 
intermediate outputs are copied, the Reducers starts to merge and sort all those
spilled files into several sorted files which will be feed to the reduce function
as we defined. This merge process is done in rounds, and the number of merge files
at each round is controlled by \texttt{mapreduce.task.io.sort.factor}, similar 
as in Map stage. Until the number of merged files are equal or less than the number 
specified by \\ \texttt{mapreduce.task.io.sort.factor}, instead of merging them 
into one file, they are directly fed into reduce function. Moreover, not all
of the intermediate data in the memory buffer are spilled into disk. Portion of 
the intermediate data are reserved in the memory, and size is controlled by the
next parameter.

\subsubsection{\texttt{mapreduce.reduce.input.buffer.percent}}

This parameter is specifying the proportion of total heap size of Reducer's JVM
used to reserve the intermediate data in memory and directly feeds the reduce
function. By default, this parameter is set to be 0, which forces all intermediate
data to the local disk and leave all memory for the computation of reduce function.
However, if the memory utilization of reduce computation is extremely light, then
we can increase this parameter to be value close to 1 to keep more intermediate
data in the memory on the reduce side and save strips to the local disk, which
will improve the job performance. In \cite{li2014mronline}, it also suggests to
set this value to be same as \texttt{mapreduce.reduce.shuffle.merge.percent}. 

\subsection{Model User Tunning Parameters}


\section{\texttt{drSpaceTime} Package}

\texttt{drSpaceTime} is a R package for spatial temporal analysis using divide 
and recombined concept. It is highly depends on three exist R packages: 
\texttt{Rhipe}, \texttt{stlplus}, and \texttt{Spaloess}, which are all open 
source and available on Github \cite{github}.
