\chapter{Multi-factor designed experiment for performance of the 
Nonparametric-Regression Modeling}

In this chapter, we shift our gears to the analysis of the performance of the 
analysis method we proposed in previous chapter for a large spatial temporal data.
There are two groups of tunning parameters for our nonparametric analysis model.
One is the tunning parameters of statistical model, such as smoothing window, 
smoothing degree for both temporal fitting and spatial fitting, which we have
already demonstrated in \cite{} and \cite{} using cross-validation method. Another
group of tunning parameters are user-tunable MapReduce tunning parameters. Within
this chapter, we illustrate a full factorial experiment to study the affect of
these system tunning parameters to the modeling.

\section{Modeling Routine}

The raw text data files as mentioned in section~\ref{sec:Download}, which is 
sitting on HDFS, is first merged into one text file, and then duplicated by
year. Specifically, the time series of monthly maximum temperature of each station
is replicated based on original 48 years observations to include $2^{18}$ years 
which is $3,145,728$ monthly observations. 
The total size of the text data file is 386 GB which includes monthly maximum
temperature observation over $3,145,728$ months at 7,738 different locations.

The whole analysis routine consist of six MapReduce jobs, which are listing as
following in order:

\begin{itemize}
\item Reading in
\begin{description}
  \item[Input] The duplicated raw text data file on HDFS is read in as input. Each
  row is read in as a key-value pair. The key is a unique row index, and value
  is the corresponding string in that row. 
  \item[Output] The value of each output key-value pair is a matrix with dimension
  7,738 by 2. Each row represents the maximum temperature and station id of one 
  location for a given month, and the key is the corresponding month index varies 
  from 1 to $3,145,728$. 
  \item[Map] Every block of the raw text data saved on HDFS is sent to one of 
  mapper with each row as one of input key-value pairs. By calling the R function
  \texttt{strsplit}, each filed of the row string is split apart. Since each
  row contains 12 monthly maximum temperature of a given year at a given location,
  we generate a intermediate key-value pair for each of them for all locations
  and all years. The key of 
  intermediate key-value pairs is the month index of corresponding row, and the 
  value is a vector with station id and maximum temperature.
  \item[Reduce] All intermediate key-value pairs that share the same month index
  are shuffled and sent to one Reducer. By calling R function \texttt{rbind}, 
  all intermediate values are row binded into one matrix with dimension of 7,738
  by 2.
\end{description}

In the Reading in MapReduce job


\item MapReduce job for spatial fitting of maximum temperature
\item MapReduce job for swapping from by time division to by location division
\item MapReduce job for temporal fitting  
\item MapReduce job for swapping from by location division to by time division
\item MapReduce job for spatial fitting of remainder
\end{itemize}

readin, spatial-fit, swaptoLoc, stlplus-fit, swaptoTime, spatial-fit

\subsection{Type of MapReudce Job}

There are three main steps in one MapReduce job: Map, Shuffle and Sort, Reduce.

Within our routine of analysis for spatial-temporal data, we categorize all 
necessary MapReduce job into two different groups. The first type of MapReduce
job is mainly focus on one of smoothing models, which is in either spatial or 
temporal dimension. We name it as $model$-$fitting$ job. For this type of job, it 
reads in a given type of division (either by time or by location) from HDFS, and 
then carries out one type of smoothing procedure, depends on what the division is,
within its Map step. There is no need for neither Reduce step nor shuffle and sort
step in this type of job since the division format is not changed. So the output 
of Map function is directly written onto HDFS.

Another type of job is focusing on switching from one division to another, which
we name as $swapping$ job. For this type of job, it reads in a given type of 
division from HDFS. And for each input key-value pair, the Map function generates
multiple intermediate key-value pairs with different keys. For instance, if by 
time division is read in, and observations of all locations in each month is one 
input key-value pair, then 7,738 intermediate key-value pairs is created with each
location ID as key in the Map function of $swap$-$to$-$location$ MapReduce job.
Then the output of Map function is first written to local disk of nodes 

\section{Experiment Design}

\subsection{Hadoop User Tuning Parameters}

\begin{itemize}
\item \texttt{mapreduce.task.io.sort.mb}

\item \texttt{mapreduce.map.sort.spill.percent}

\item \texttt{mapreduce.reduce.shuffle.parallelcopies}

\item \texttt{mapreduce.task.io.sort.factor}

\item \texttt{mapreduce.reduce.shuffle.merge.percent}

\item \texttt{mapreduce.reduce.merge.inmem.threshold}

\item \texttt{mapreduce.reduce.input.buffer.percent}

\item \texttt{mapreduce.reduce.shuffle.input.buffer.percent}

\item \texttt{mapreduce.job.reduce.slowstart.completedmaps}

\end{itemize}


\subsection{Model User Tunning Parameters}


\section{\texttt{drSpaceTime} Package}

\texttt{drSpaceTime} is a R package for spatial temporal analysis using divide 
and recombined concept. It is highly depends on three exist R packages: 
\texttt{Rhipe}, \texttt{stlplus}, and \texttt{Spaloess}, which are all open 
source and available on Github \cite{github}.