\chapter{Multi-factor designed experiment for performance of the 
Nonparametric-Regression Modeling}

In this chapter, we shift our gears to the analysis of the performance of the 
analysis method we proposed in previous chapter for a large spatial temporal data.
There are two groups of tunning parameters for our nonparametric analysis model.
One is the tunning parameters of statistical model, such as smoothing window, 
smoothing degree for both temporal fitting and spatial fitting, which we have
already demonstrated in \cite{} and \cite{} using cross-validation method. Another
group of tunning parameters are user-tunable MapReduce tunning parameters. Within
this chapter, we illustrate a full factorial experiment to study the affect of
these system tunning parameters to the modeling.

\section{Type of MapReudce Job}

There are three main steps in one MapReduce job: Map, Shuffle and Sort, Reduce.

Within our routine of analysis for spatial-temporal data, we categorize all 
necessary MapReduce job into two different groups. The first type of MapReduce
job is mainly focus on one of smoothing models, which is in either spatial or 
temporal dimension. We name it as $model$-$fitting$ job. For this type of job, it 
reads in a given type of division (either by time or by location) from HDFS, and 
then carries out one type of smoothing procedure, depends on what the division is,
within its Map step. There is no need for neither Reduce step nor shuffle and sort
step in this type of job since the division format is not changed. So the output 
of Map function is directly written onto HDFS.

Another type of job is focusing on switching from one division to another, which
we name as $swapping$ job. For this type of job, it reads in a given type of 
division from HDFS. And for each input key-value pair, the Map function generates
multiple intermediate key-value pairs with different keys. For instance, if by 
time division is read in, and observations of all locations in each month is one 
input key-value pair, then 7,738 intermediate key-value pairs is created with each
location ID as key in the Map function of $swap$-$to$-$location$ MapReduce job.
Then the output of Map function is first written to local disk of nodes 

\section{Modeling Routine}

The whole analysis routine consist of six MapReduce jobs, which are listing as
following in order:

\subsection{Reading in}

The first MapReduce job in the modeling routine is the Reading in job.
The raw text data files as mentioned in section~\ref{sec:Download}, which is 
sitting on HDFS, is first merged into one text file, and then duplicated by
year. Specifically, the time series of monthly maximum temperature of each station
is replicated based on original 48 years observations to include $2^{18}$ years 
which is $3,145,728$ monthly observations. 
The total size of the text data file is 386 GB which includes monthly maximum
temperature observation over $3,145,728$ months at 7,738 different locations.

\begin{description}
  \item[Input] The duplicated raw text data file on HDFS is read in as input. Each
  row is read in as a key-value pair. The key is a unique row index, and value
  is the corresponding string in that row. 
  \item[Output] The value of each output key-value pair is a matrix with dimension
  7,738 by 2. Each row represents the maximum temperature and station id of one 
  location for a given month, and the key is the corresponding month index varies 
  from 1 to $3,145,728$. 
  \item[Map] Every block of the raw text data saved on HDFS is sent to one of 
  mapper with each row as one of input key-value pairs. By calling the R function
  \texttt{strsplit}, each filed of the row string is split apart. Since each
  row contains 12 monthly maximum temperature of a given year at a given location,
  we generate a intermediate key-value pair for each of them for all locations
  and all years. The key of 
  intermediate key-value pairs is the month index of corresponding row, and the 
  value is a vector with station id and maximum temperature.
  \item[Reduce] All intermediate key-value pairs that share the same month index
  are shuffled and sent to one Reducer. By calling R function \texttt{rbind}, 
  all intermediate values are row-binded into one matrix with dimension of 7,738
  by 2. Finally there are 3,145,728 key-value pairs generated and saved on HDFS.
\end{description}

Notice that the computation complexity of each Mapper is very light, mainly just
\texttt{strsplit} and \texttt{rhcollect} function are called. On the other hand,
the size of output data of each Mapper is roughly close the input data. In other
words, a lot of intermediate key-value pairs, in term of size, are shuffled and
sorted and then sent to corresponding Reducers. For this type of job, the best 
performance can be obtained by assigning more memory from heap size of each JVM 
to use for shuffle and sorting, which can avoid multiple meaningless I/O to disk.  
On the reduce side, it is also very light with respect to computation. Only 
\texttt{rbind} function is called repeatedly. So the best performance is obtained 
when the intermediate data can reside entirely in memory of the Reducer JVM, 
which achieved by assigning more memory from the JVM of Reducer for the purpose
of holding as many as possible intermediate key-value pairs in memory. 

\subsection{Spatial Fitting of Original Observation}

By taking the subset by time generated by the job of reading in, the second 
MapReduce job is applying spatial loess smoothing fitting to each month 
independently in parallel. Meanwhile, a RData file which contains meta-data such
as longitude, latitude, and elevation about all 7,738 stations are saved on HDFS.
These information is used for each spatial loess smoothing in the Map step. 

\begin{description}
  \item[Input] 3,145,728 key-value pairs generated in the first reading in job.
  The key is the month index, and the value is a matrix with dimension of 7,738
  by 2. First column is location index, and another column is the corresponding 
  maximum temperature observation.
  \item[Output] The number of output key-value pairs is still 3,145,728. The key
  is still the month index, but the value is a vector of spatial loess fitted
  value with length 7,738, one for each location. The order of fitted value in 
  each vector is kept as same based on the order of locations. So we can get rid
  of the location information from the output key-value pairs.
  \item[Map] The shared RData file containing all location information is first 
  copied and load into the global environment of each Mapper. The input matrix
  is merged with station information R object (a data.frame with 7,738 rows and
  \texttt{lon}, \texttt{lat}, and \texttt{elev} columns). Then \texttt{spaloess}
  function from package \texttt{Spaloess} is called to calculate the spatial
  loess smoothing value for each station at given month. The key of each 
  intermediate key-value pair is month index, and value is vector of spatial 
  smoothing value in the same order of locations. 
  \item[Reduce] There is no Reduce need for this job. The intermediate key-value
  pairs are directly written to HDFS.
\end{description}

This particular MapReduce job is different than the first reading in job. Here
the Reduce step is not necessary. Moreover, the shuffle and sorting stage should
also be avoid to save unnecessary network traffic and multiple trips to the disk.
Consequently, a lot of MapReduce tuning parameter is not needed to be considered
in this job. The best performance of this job can be obtained by enlarging the 
memory assigned to each Mapper for the computation.

\subsection{Swapping from By-time Division to By-location Division}

After the spatial smoothing in each month, we have to switch the data from by 
each month division to by each location division. 



\subsection{Temporal Fitting}

\subsection{Swapping from by location division to by time division}

\subsection{Spatial Fitting of Remainder}

readin, spatial-fit, swaptoLoc, stlplus-fit, swaptoTime, spatial-fit



\section{Experiment Design}

\subsection{Hadoop User Tuning Parameters}

\begin{itemize}

\item \texttt{mapreduce.task.io.sort.mb} \\
This parameter controls the size of memory buffer, in megabytes, which is used 
to hold the map output in each Mapper. The value for it is integer varies from 1 
to 2047. The default value for this parameter is 100, which only allocates 100MB
of memory from heap size of JVM for saving map output. It is quite small in 
general. For jobs like reading in and swapping, it is definitely worth to increase
the value of this parameter to give more memory for holding output of each Mapper.

\item \texttt{mapreduce.map.sort.spill.percent}\\
This parameter works with \texttt{mapreduce.task.io.sort.mb} collectively to 
control the memory used for holding map output. Concretely, a circular memory 
buffer is allocated for each mapper to write intermediate output to. When the 
map output occupied \texttt{mapreduce.map.sort.spill.percent} percent of \\
\texttt{mapreduce.task.io.sort.mb}, the output is spilled to local disk of node
where Mapper is running. Meanwhile the Mapper is keeping writing output to this
circular memory buffer when spilling is proceeding. However if the memory buffer
is filled up during this time period, the Mapper is paused until the spill is
finished. Clearly, it is critical to set this parameter as well as the total 
amount of memory buffer to be high enough if there are numerous amount of output
generated by each Mapper. However, we do not want set them to be too large.
Because the memory buffer is still belongs to the Mapper JVM heap size, which is
decided by \texttt{mapreduce.map.java.opts}. Reserving too much memory from the
total heap size of JVM will leave limited memory usage for other processes sponsed
by the JVM, and indeed will force JVM to involve more garbage clean, which in 
turn will hurt the job performance.    

\item \texttt{mapreduce.job.reduce.slowstart.completedmaps}\\
The parameter controls when Reducers should be launched. Specifically, it specifies
the fraction of the number of Mappers which should be complete before Reducers
are scheduled for the job.

\item \texttt{mapreduce.reduce.shuffle.parallelcopies}\\
After the map step is finished, the intermediate key-value pairs are partitioned
and sitting on the local disk of the node where Mapper ran. Then each Reducer 
the corresponding partition from all Mapper outputs through the network. And
actually each Reducer evokes a number of copy threads, which controlled by this
parameter, to fetch the intermediate output of Mapper in parallel. This parameter
should be set wisely. Too large value of copy threads will waste for the CPU,
but too small number of copy threads will slow down the copy step of Reducer.
The default value is 5.




\item \texttt{mapreduce.task.io.sort.factor}

\item \texttt{mapreduce.reduce.shuffle.merge.percent}

\item \texttt{mapreduce.reduce.merge.inmem.threshold}

\item \texttt{mapreduce.reduce.input.buffer.percent}

\item \texttt{mapreduce.reduce.shuffle.input.buffer.percent}

\item \texttt{mapreduce.job.reduce.slowstart.completedmaps}

\end{itemize}


\subsection{Model User Tunning Parameters}


\section{\texttt{drSpaceTime} Package}

\texttt{drSpaceTime} is a R package for spatial temporal analysis using divide 
and recombined concept. It is highly depends on three exist R packages: 
\texttt{Rhipe}, \texttt{stlplus}, and \texttt{Spaloess}, which are all open 
source and available on Github \cite{github}.