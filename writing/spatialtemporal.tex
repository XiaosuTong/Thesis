\chapter[DIVIDE AND RECOMBINED ANALYSIS TO SPATIAL-TEMPROAL \\ DATA]{DIVIDE AND RECOMBINED ANALYSIS TO SPATIAL-TEMPROAL DATA}

\section{Source of the Data}

Historical records of weather such as monthly precipitation and temperatures from 
the last century are an invaluable database to study changes and variability in 
climate. These data also provide the starting point for understanding and modeling 
the relationship among climate, ecological processes and human activities. 

Because of digitization and rapid development of computer hardware, nowadays
people are able to store, summarize and analyze larger and more 
complex climate data set than before, which stimulates the dramatically 
increasing  of the demand and interest of high-value environmental data and 
information. In United States, National Centers for Environmental Information, 
formerly the National Climatic Data Center (NCDC), is responsible for hosing and 
providing access to one of the most significant archives of environmental 
information. Some of data set hosted by NCEI \cite{NCEI} are shown as follows:

\begin{itemize}
  \item COOP:
    Through the National Weather Service (NWS) Cooperative Observer Program 
    (COOP), more than 10,000 volunteers take daily weather observations at 
    National Parks, seashores, mountaintops, and farms as well as in urban and 
    suburban areas. COOP data usually consist of daily maximum and minimum 
    temperatures \cite{COOP}.
  \item SNOTEL:
    The Natural Resources Conservation Service (NRCS) operates and maintains an 
    extensive and automated system (SNOwpack TELemetry or SNOTEL) designed to
    collect snowpack and related climatic data like air temperature in the Western
    United States begins in 1978 \cite{SNOTEL}.
  \item AG:
    Agricultural climate data for southeastern Washington from United States
    Department of Agriculture Natural Resources Conservation Service (USDA-NRCS) 
    \cite{USDA}.
  \item MRCC:
    Midwest Climate Data Center data, mainly for period between 1895 and 1948. 
    The MRCC \cite{MRCC} serves the nine-state Midwest region (Illinois, Indiana, 
    Iowa, Kentucky, Michigan, Minnesota, Missouri, Ohio, and Wisconsin).
  \item USHCN:
    Historical Climate Network data provides summary of the month temperature and 
    precipitation observations for 1,218 stations across the contiguous United 
    States. Temperature observations have been homogeneity corrected to remove 
    biases associated with non-climatic influences, such as changes in 
    instrumentation and observing practices, and changes to the environment 
    including station relocations \cite{USHCN}.
\end{itemize}

In 2002, based on the data set we listed above, the NCEI has further
processed them to produce a consolidated and uniform 103-year spatial temporal
data set by combining stations at similar locations, eliminating stations with
short records, and aggregating some of daily measurement to be monthly.
In summary, the data we are going to analyze is about observed monthly average
maximum daily temperatures for the conterminous US from 1895 to 1997. There are
8,125 stations reporting monthly average maximum daily temperatures at some time
in this period which includes 1,236 months. 

\section{Exploratory Analysis}

The FTP distribution for the 103-year monthly maximum temperature data set along
with supporting meta-data can be found at \cite{data}. 
However, the data set not only includes the raw observed monthly
maximum temperature, but also infills all missing values by statistical infilled
method conducted by IMAGe. 
When using the complete data for further statistical analysis care should be taken
with the infilled values. Although they are good estimates of the mean they may 
not reproduce the variability that one would expect from actual point 
observations of the meteorology. In statistical language, the infilled values are
the mean of the conditional distribution for the measurement given the observed 
data. They are not samples from this conditional distribution. 
So we eliminate all infilled values from our data analysis.

In this section, we describe the procedures of our data downloading and initial 
processing, followed by summary statistics of the observed monthly maximum 
temperature. Further processing of the data into multiple databases of various 
structures are discussed in details in the following sections along with the 
corresponding analyses enabled and facilitated by each database.

\subsection{Data Download and Initial Database}
\label{sec:Download}

The raw data set is in fixed width text format with 103 files having file names in
the form: 
\texttt{tmax.complete.Ynnn} with \texttt{nnn = 001, 002,} $\cdots$\texttt{, 103} 
which \texttt{001 = 1895} and \texttt{103 = 1997}. Each separate data file consists
of the maximum temperature for a single year. Each line of the file contains 12
monthly maximum temperature observations and 12 observed/missing value codes 
(\texttt{1=missing}, \texttt{0=observed}). We only keep the observed monthly
maximum temperature in our analysis. Totally there are 8,125 stations in each file,
and temperature appears as a integer in tenths of Celsius degree. For instance, 73
in the raw text file should be interpreted as 7.3 Celsius degrees. 

Additionally, besides the 103 raw data files, there is metadata about stations' 
information which includes:
\begin{enumerate}
  \item The station id, which uniquely identifies each station.
  \item The degree of longitude of station.
  \item The degree of latitude of station.
  \item The meter of elevation of station.
\end{enumerate}
The metadata set is in text format of 244 KB.

The first step of our 
data downloading is to parallelly download 103 data files from IMAGe data base and 
transferred from the local file system of the cluster to the HDFS. This is done
through a MapReduce job described as follow.
\begin{description}
  \item[Input] Integers from 1 to 103. There is no real input files feed into this
  MapReduce job. Instead, we simulate the key of input key-value pairs to be
  sequence of integer from 1 to 103 which is exactly the number of data set files. 
  Each of them is passed to one Mapper. 
  \item[Output] The same text files on HDFS.
  \item[Map] Download one file in each mapper. For each Mapper, the download link
  is generated by concatenating the string\\
  \texttt{https://www.image.ucar.edu/pub/nychka/NCAR\_tinfill/tmax.complete.Y}
  with the integer passed into this Mapper. The corresponding data file is
  downloaded into a temporal directory on the local file system of the server on 
  which the Mapper is running, and then this local copy of file is copied 
  into HDFS after Map is finished.  
  \item[Reduce] Reduce is not needed in this MapReduce job.
\end{description}
The total raw text files saved on HDFS are around 90MB. Totally there are 836,875 
lines in text files, and each line of the raw text file corresponded to twelve
monthly maximum temperature observations for a given station in a given year
followed by their status code of observation (observed or missing) in terms of 0/1.

\subsubsection{Construction of the Initial Database}

Next, an initial database of the whole data set is created based on plain text
files. We consider to transform the raw text files to be a more
analysis friendly structure, which only has one observation per row. Concretely,
the new structure should include following fields (columns) for each observation.
We use \texttt{NA} to represent missing value of the maximum temperature for a
given month:

\begin{enumerate}
  \item The station ID, which uniquely identifies each station.
  \item The degree of longitude of the station.
  \item The degree of latitude of the station.
  \item The meter of elevation of the station.
  \item The station name.
  \item The year of the observation.
  \item The month of the observation.
  \item The observation of the maximum temperature for the given month.
\end{enumerate}

By utilizing the RHIPE, which bridges the R and Hadoop, the new data structure can
be easily achieved within one MapReduce job by specifying RHIPE commands in R.
Moreover, each observation is saved in terms of R object, specifically a data.frame
with only one row and eight columns.
We make the choice of R data.frame because it allows us to have different type of
class for each column, and also it is extremely friendly to a great many of
statistical analysis functions in R. 

The observation database is fulfilled in a MapReduce job described as
below:

\begin{description}
  \item[Input] 103 plain text files saved on HDFS, each of which consists of 8,125 
  lines. Each line includes information about 12 months maximum temperature for a 
  given station in a given year. The year information can be pulled from last three
  characters of the name of each text file, which \texttt{001 = 1895} and 
  \texttt{103 = 1997}. Lines contain 14 tab-delimited columns:
  (1) the station id, (2) to (13) the monthly maximum temperature from January to
  December, (14) a 12-digit character which indicates if 12 monthly maximum 
  temperature is observed or missing. 
  \item[Output] 10,042,500 observations, each of which is a key-value pair, with 
  8,125 unique station ID and 1,236 month index together as the key, and a one-row 
  R data.frame as value, which includes (1) 
  \texttt{station name}, the name of station, (2) \texttt{longitude}, degree of 
  longitude of the station, (3) \texttt{latitude}, the latitude of the station, (4)
  \texttt{elevation}, the meter of elevation the station, (5) \texttt{year}, year 
  of the observation, (6) \texttt{month}, month of year (January to December), 
  (7) \texttt{tmax}, the monthly maximum temperature
  \item[Map] Parse each line of the text file as a character string. For each line,
  we split the character string based on separator \texttt{\textbackslash t} by 
  calling R function \texttt{strsplit}, and then the station ID in the first field 
  is extracted out, and with the month index together are used as the key of the 
  intermediate key-value pairs. The rest of fields are saved in a R one-row 
  data.frame as the value of the intermediate key-value pairs. The intermediate 
  key-value pairs are transferred to Reduce. 
  \item[Reduce] Identity reduce function do not do anything to the intermediate 
  key-value pairs besides evenly distributes all intermediate key-value pairs to 
  multiple files, and save them on HDFS.
\end{description}

\subsection{Summary of the Data}
\label{sec:summaryData}

In summary, the dataset covers monthly maximum temperature over the period of 103
years, from Jan 1895 to Dec 1997. 8,125 stations are scattered across the 
conterminous US excluding Hawaii and Alaska. Concretely, the elevation of 
stations ranges from -59 to 3801 meters, the longitude ranges from -124.73 degree
to -67 degree, and the latitude ranges from 24.55 degree to 49 degree. 

\subsubsection{Location of Stations}

Figure \href{../plots/allstationsone.pdf}{\ref*{all.location}} illustrates the
location of all stations on the map of United States.

\begin{framed}
\begin{center}
  \href{../plots/allstationsone.pdf}{Link to figure}
  \captionof{figure}{The location of all stations}
  \label{all.location}
\end{center}
\end{framed}

Meanwhile, we equally split the stations into 8 groups based on their elevation, 
which cutting points are at 58m, 162m, 244m, 351m, 581m, 1098m, and 1647m. And 
then location of stations are demonstrated on map of United States conditional on 
the elevation.

\begin{framed}
\begin{center}
  \href{../plots/allstations.pdf}{Link to figure}
  \captionof{figure}{The location of stations conditional on elevation}
  \label{all.location.multi}
\end{center}
\end{framed}

Besides the demonstration of stations based on longitude and latitude, the 
distribution of elevation of each station is also shown in Figure 
\href{../plots/QQelevation.pdf}{\ref*{QQelevation}}.

\begin{framed}
\begin{center}
  \href{../plots/QQelevation.pdf}{Link to figure}
  \captionof{figure}{Quantiles of elevation of stations}
  \label{QQelevation}
\end{center}
\end{framed}

In Figure \href{../plots/QQelevation.pdf}{\ref*{QQelevation}}, horizontal dash 
lines are the cutting points of elevation when we split stations into groups. 
It shows that more than
60\% of stations are below 581 meters. Figure 
\href{../plots/allstations.pdf}{\ref*{all.location.multi}} tells us that those 
stations are mainly outside of the 
Pacific Coast Ranges (officially gazetted as the Pacific Mountain System in the 
United States), which includes the Rocky Mountains, Columbia Mountains, Interior
Mountains, the Interior Plateau, Sierra Nevada Mountains, the Great Basin mountain 
ranges. Also, there are 724 station are on the west coast of United States, which
is between the Pacific Ocean and the Pacific Coast Ranges.

\subsubsection{The Number of Observation}

As we mentioned in section~\ref{sec:Download}, missing values has been saved as 
\texttt{NA} in the database. We would like to reveal where and when those 
missing values turn up.
In Figure \href{../plots/obs_month.pdf}{\ref*{obs.month}}, the log base 2 number
of observation is drawn against date. The red solid reference line in the plot is 
the $\log_2(8125)$, which is the number of observation when there is no missing 
value in that given month. The number of observation increased gradually from Jan
1895 to Jan 1915. But then there was a dramatically jump in 1931. The valid
observation number arose from 1,619 on Dec 1930 to 2,953 on Jan 1931. After Jan 
1950, the count of observation stayed around $5,800$, which is 71.3\% of all
stations. 

\begin{framed}
\begin{center}
  \href{../plots/obs_month.pdf}{Link to figure}
  \captionof{figure}{The number of observations over time}
  \label{obs.month}
\end{center}
\end{framed}

With a deeper look into the database, we found January has minimum number of 
observation in 70 out of 103 years. So the observation count is plotted against
date superposed on month of year, only for January, February, and December in 
Figure \href{../plots/obs_month_byseason.pdf}{\ref*{obs.month.season}}

\begin{framed}
\begin{center}
  \href{../plots/obs_month_byseason.pdf}{Link to figure}
  \captionof{figure}{The number of observations around Jan 1982}
  \label{obs.month.season}
\end{center}
\end{framed}

There is not huge difference between 12 month of year with respect to the 
observation count before 1981. However, Figure 
\href{../plots/obs_month.pdf}{\ref*{obs.month}} and 
Figure \href{../plots/obs_month_byseason.pdf}{\ref*{obs.month.season}} both 
indicate there is a significant drop of observation count on Jan 1982. And among 
the 16 years after that, January always has the lowest observation count.

Moreover, we found that this drop on Jan 1982 is not caused by adjustment of 
the stations such as one station shift its location to its neighborhood. It is
because the 688 stations got missing observation on Jan 1982, but then they
went back to be active again on Feb 1982, as shown in Figure 
\href{../plots/obs_Jan1982.pdf}{\ref*{Jan1982}}. The blue circles represent 
stations whose observation status were changed to be missing from December 1981
to January 1982. Meanwhile the magenta solid points represent stations whose
observation status shifted back to be valid from January 1982 to February 1982.
If a location has both blue circle and magenta solid point, it means that location
of weather station suddenly was failing on January 1982 but then went back on
February 1982. 

\begin{framed}
\begin{center}
  \href{../plots/obs_Jan1982.pdf}{Link to figure}
  \captionof{figure}{The location of active stations around Jan 1982}
  \label{Jan1982}
\end{center}
\end{framed}

We believe this astonishing number of failure of stations on January 1982 is caused
by severe weather on January 1982, which may cause the battery in the weather 
station died. As shown in Figure \href{../plots/monthmean.pdf}{\ref*{min.monthmean}},
the mean of monthly minimum temperature over all stations are plotted against 
date from 1931 to 1998. We found on January 1982, it reached the minimum value 
of $-15.65$. And this is only for stations who has record in that month, there 
were a lot of stations had missing value may have even lower records.  
From January 11 to January 17, A brutal cold snap sends temperatures to all-time 
record lows in dozens of cities throughout the Midwestern United States. Especially
on January 17, 1982, so called "Cold Sunday"\cite{coldsunday}, in numerous cities 
temperatures fall to their lowest levels in over 100 years.

\begin{framed}
\begin{center}
  \href{../plots/monthmean.pdf}{Link to figure}
  \captionof{figure}{The monthly minimum temperature vs. date}
  \label{min.monthmean}
\end{center}
\end{framed}

\section{Monthly Maximum Temperature without Missing Value}

Before we jump into the analysis of spatial and temporal dimension interactively, 
we start our analysis with the time series analysis on each station independently.
In this section we consider the stations without missing value, which
there are 432 stations in total. The for visualization purpose, we randomly sample
100 stations out of them. Their locations are shown as in Figure 
\href{../plots/100stations.pdf}{\ref*{100stations}}. For each station, we use 
STL (Seasonal Trend Loess) method to analysis the time series. 
Additionally, we would like to carry out the analysis for each station in parallel. 

\begin{framed}
\begin{center}
  \href{../plots/100stations.pdf}{Link to figure}
  \captionof{figure}{The location of the 100 stations}
  \label{100stations}
\end{center}
\end{framed} 

\subsection{Division by Station}
\label{sec:divibyStation}

In order to analyze multiple time series in parallel, we need a database or division,
which groups observations of one station all together. And then STL method is 
applied to each station independently. Finally, the results of decomposition from
STL method needs to be readily presented by visualization method so that we can
compare the results cross multiple stations. 

Fortunately, by using RHIPE, which integrates the R and Hadoop, the division by 
station and the STL analysis on each station can be handily accomplished.
On the one hand, R provides analysis friendly data structure and powerful 
statistical analysis and visualization packages such as STL+, lattice package.
On the other hand, Hadoop enables the efficient processing of divisions, such as
subsets of division by station can be represented as key-value pairs saved on 
HDFS. In the following, we describe in details the procedures to construct the 
division by station database via RHIPE on a cluster of 11 servers and 242 processors.

\begin{description}
  \item[Input] 10,042,500 key-value pairs from the initial database, with 8,125 
  \texttt{station.id} and 1,236 months as the key, and a one-row R data.frame 
  as the value. The value includes (1) \texttt{station.name}, the name of station, 
  (2) \texttt{longitude}, degree of longitude of the station, (3) \texttt{latitude}, 
  the latitude of the station, (4) \texttt{elevation}, the meter of elevation the 
  station, (5) \texttt{year}, year of the observation, (6) \texttt{month}, month 
  of year (January to December), (7) \texttt{tmax}, the monthly maximum temperature.
  \item[Output] division by station in form of 432 key-value pairs, with 
  \texttt{station.id} as the key, and R data.frame containing 1,236 observations
  information of corresponding station as the value.
  \item[Map]For every input key-value pair, we do not carry out any adjustment on
  the one-row data.frame of value. But we do change the key from a vector of 
  \texttt{station.id} and month index to \texttt{station.id} only. 
  \item[Reduce] 1,236 of one-row data.frame corresponding to one station are 
  aggregated to be one data.frame. Concretely all one-row data.frame who share
  the same station ID are shuffled and transferred to one reducer, and then R function 
  \texttt{rbind} is called in that reducer to combine those one-row data.frame, 
  which includes (1) \texttt{year}, year of the observation, (2) \texttt{month}, 
  month of year (January to December), (3) \texttt{tmax}, the monthly maximum 
  temperature. The rest of information about the station, \texttt{station.name}, 
  \texttt{latitude}, \texttt{longitude}, \texttt{elevation}, is save as an attributes associated with the data.frame named \texttt{location}.
\end{description}

\subsection{Time Series Analysis of Each Subset}

After we created the database of division by station, we are ready for the parallel 
analysis of all subsets of the database. The analysis method we apply to each station
is Seasonal Trend Loess (SLT). Specifically, we utilize R package named stl2\cite{stl2},
which provides several enhancements including the ability to deal with missing values 
and higher order polynomial smoothing compared to the stl method that ships with 
base R. Then we collect the analysis result of 100 randomly sampled stations together 
to carry out the visualization. We detail the process of corresponding MapReduce 
job below.

\begin{description}
  \item[Input] 432 key-value pairs from the division by station database, with 432 
  unique \texttt{station.id} as the key, and a R data.frame as the value. The value 
  includes (1) \texttt{year}, year of the observation, (2) \texttt{month}, month 
  of year (January to December), (3) \texttt{tmax}, the monthly maximum temperature.
  And an attributes named \texttt{location} encompassing \texttt{station.name},
  \texttt{latitude}, \texttt{longitude}, and \texttt{elevation} is attached to the
  data.frame.
  \item[Output] One key-value pair with 1 as the key, and R data.frame with 123,600
  rows and 11 columns, which contains \texttt{tmax}, seasonal components, trend 
  components, and remainders, \texttt{year}, \texttt{month}, and station location 
  information: \texttt{latitude}, \texttt{longitude}, \texttt{elevation}. 
  \item[Map]For every input key-value pair, we first order the 1,236 observations 
  of one station by month index from 1 to 1,236. Then we apply STL method on the 
  time series of maximum temperature. The fitting result is also saved in a 
  data.frame. Next, we add a new column to the data.frame which is the corresponding 
  \texttt{station.id}. Next, we filter out key-value pairs whose key 
  \texttt{station.id} is not belong in that 100 stations. Finally, all keys are 
  replaced to be 1 so that fitting results of all 100 stations can be accumulated 
  into one data.frame in the reducer. \item[Reduce] 100 intermediate key-value 
  pairs who share the same key 1 are transfered into one reducer. In the reduce 
  function, they are combined together by calling R function \texttt{rbind} to 
  generate the final one data.frame. 
\end{description}

As shown in the reduce function above, a set of predefined parameter setting for 
STL and a string vector of the 100 sampled \texttt{station.id} are passed into 
the MapReduce job by utilizing \texttt{parameters} argument in \texttt{rhwatch} 
function, which is used to share R objects in the Global Environment of R session 
on the front-end server to all workers of the MapReduce job. After the fitting, 
we would like to visually check the model validation, with same parameter setting, 
across all different stations.

\subsubsection{First Run}

In the first run of parallel STL fit, we specify the parameters to be: 
\texttt{t.window} is 495, \texttt{t.degree} is quadratic, \texttt{s.window} is 
77, and \texttt{s.degree} is linear, \texttt{inner} is 10. The first thing we would 
like to check about with respect to the STL fitting is comparing the fitted value 
with the raw observations. In Figure 
\href{../plots/100stations/first_run/fitted.100stations.tmax.pdf}
{\ref*{firstrun.fitted.100stations}}, each page is one station. The whole time 
series with 1,236 months for one station is chunked into 9 periods. Each of the 
first 8 periods has 144 monthly observations, the last period has 84 monthly 
observations. The raw observations are drawn with blue points, and the seasonal 
component plus trend component are drawn with magenta curve.  

\begin{framed}
\begin{center}
  \href{../plots/100stations/first_run/fitted.100stations.tmax.pdf}{Link to figure}
  \captionof{figure}{The fitted value vs. month}
  \label{firstrun.fitted.100stations}
\end{center}
\end{framed}

Clearly, no matter where the station is, the fitted values (seasonal+trend) are 
very close to the true values, and also they capture the seasonal pattern in the 
raw data. Next, we carry out the diagnostic for each components based on visualization.

\textbf{Trend Components}

In Figure \href{../plots/100stations/first_run/trend.100stations.tmax.pdf}
{\ref*{firstrun.trend.100stations}}, the trend component is drawn against to month
index in blue curve conditional on \texttt{station.id}. So each panel is about 
the trend component of one station. Meanwhile, we plot with trend components 
the moving average of yearly mean of monthly maximum temperature.
Collectively, the same STL parameter setting still can capture the different trend
behavior across all stations.

\begin{framed}
\begin{center}
  \href{../plots/100stations/first_run/trend.100stations.tmax.pdf}{Link to figure}
  \captionof{figure}{The trend components vs. month index}
  \label{firstrun.trend.100stations}
\end{center}
\end{framed}

\textbf{Seasonal Components}

The diagnostic plot for seasonal component is shown in Figure 
\href{../plots/100stations/first_run/sea+remaind.month.100stations.tmax.pdf}
{\ref*{firstrun.searemaind.100stations}}, named seasonal-diagnostic plots, which
helps us decide how much of the variation in the data other than trend component
should go into the seasonal component and how much into the remainder.

\begin{framed}
\begin{center}
  \href{../plots/100stations/first_run/sea+remaind.month.100stations.tmax.pdf}
  {Link to figure}
  \captionof{figure}{The sum of seasonal components and remainders vs. year}
  \label{firstrun.searemaind.100stations}
\end{center}
\end{framed}

Seasonal component and remainders of
one station are plotted against to year on one page conditional on month of year
(Jan to Dec). Let $\bar S_k$ be the mean of the seasonal component for the $k$-th 
month of year. The magenta curve in the panel for the $k$-th month of year is 
seasonal components subtract out their mean $\bar S_k$. The blue points are seasonal
components plus remainders for $k$-th month of year minus their mean $\bar S_k$ 
as well. The reason for subtracting $\bar S_k$ is to center the values on each 
panel at zero; note that the vertical scales of all panels of all stations are 
the same so that we can graphically compare the variation of values on different 
panels.

\textbf{Remainders}

The first figure about remainder is shown in Figure 
\href{../plots/100stations/first_run/remainder.month.100stations.tmax.pdf}
{\ref*{firstrun.remaind.100stations}}. Remainder of each station is graphed against
to year conditional on month of year. Also a loess smoothing curve with span equal
to 0.75 is plotted for each month of year. A flat loess smoothing line represents
the seasonal component does capture reasonable amount of variation in each 
sub-series.

\begin{framed}
\begin{center}
  \href{../plots/100stations/first_run/remainder.month.100stations.tmax.pdf}
  {Link to figure}
  \captionof{figure}{The remainders vs. year}
  \label{firstrun.remaind.100stations}
\end{center}
\end{framed}

\begin{framed}
\begin{center}
  \href{../plots/100stations/first_run/QQ.remainder.100stations.tmax.pdf}
  {Link to figure}
  \captionof{figure}{The normal quantiles of remainders}
  \label{firstrun.QQremaind.100stations}
\end{center}
\end{framed}

\begin{framed}
\begin{center}
  \href{../plots/100stations/first_run/remainder.acf.100stations.tmax.pdf}
  {Link to figure}
  \captionof{figure}{Autocorrelation functions for remainder term}
  \label{firstrun.remaindacf.100stations}
\end{center}
\end{framed}

As in Figure 
\href{../plots/100stations/first_run/remainder.acf.100stations.tmax.pdf}
{\ref*{firstrun.remaindacf.100stations}},
although there are a few slightly significant autocorrelation estimates for some
of stations, surprisingly the overall behavior is indicative of independence.


\subsubsection{Second Run}

In the first run of parallel STL fit, we specify that parameters to be: 
\texttt{t.window} is 495, \texttt{t.degree} is quadratic, \texttt{s.window} is 
77, and \texttt{s.degree} is linear, \texttt{inner} is 10. However we randomly 
sample another 100 stations to be in our visualization checking because we would 
like to see if parallel STL fitting is robust cross different parameter setting 
and different locations.


\section{Monthly Maximum Temperature after 1950}

In the previous section, we illustrated how to apply STL on 432 stations in parallel
with same STL parameter setting by utilizing RHIPE computational framework. In 
this coming section, we start with a demonstration of experiments of model selection,
which select the best of parameter setting of STL model cross stations. Then we
explore the spatial loess fit for a given month. 

Recall in the Figure \href{../plots/obs_month.pdf}{\ref*{obs.month}}, we notice
the number of active stations gradually stabilized after Jan 1950. Since long
period of missing observation is not the main concern of this thesis, we decide
only consider stations with observations after Jan 1950.

\subsection{Database of Division by Month}

The first database we create for data after 1950 is called division by month. In
the following paragraph we describe the generation of such database with details.

\begin{description}
  \item[Input] 10,042,500 key-value pairs from the initial database, key is the
  vector with one of 8,125 unique \texttt{station.id} and one of 1,236 month 
  index, and a one-row R data.frame as the value. The value includes 
  (1) \texttt{station.name}, the name of station, (2) \texttt{longitude}, degree 
  of longitude of the station, (3) \texttt{latitude}, the latitude of the station, 
  (4) \texttt{elevation}, the meter of elevation the station, (5) \texttt{year}, 
  year of the observation, (6) \texttt{month}, month of year (January to 
  December), (7) \texttt{tmax}, the monthly maximum temperature.
  \item[Output] division by month in form of 576 key-value pairs, with vector of
  \texttt{year} and \texttt{month} as the key, a R data.frame containing 7,738 
  observations corresponding month as the value.
  \item[Map]For every input key-value pair, we include a new column named 
  \texttt{station.id} to the data.frame, and remove the \texttt{year} and 
  \texttt{month} from the value to be the key of intermediate key-value pairs. 
  \item[Reduce] 7,738 of one-row data.frame corresponding to one month are 
  aggregated to be one data.frame. Specifically all one-row data.frame who share
  the same \texttt{year} and \texttt{month} are shuffled and transferred to one 
  reducer, and then R function \texttt{rbind} is called in that reducer to combine 
  those one-row data.frame.
\end{description}


For each month, it is a spatial regularization of observation of maximum temperature 
over 7,738 locations. But around 2,000 different stations have missing value in 
each month. For instance, in Figure 
\href{../plots/tmax.a1950.status.pdf}{\ref*{a1950.status}},
we illustrate the observation status of 7,738 stations for 100 random sampled 
months.

\begin{framed}
\begin{center}
  \href{../plots/tmax.a1950.status.pdf}
  {Link to figure}
  \captionof{figure}{The observation status of stations on 100 months}
  \label{a1950.status}
\end{center}
\end{framed}


\subsection{Spatial Analysis without Elevation}

\subsubsection{Distance Calculation of Neighbors}

As we mentioned in section~\ref{sec:loess}, Loess method calculates the weights 
$\omega_i(x)$
for weighted least square based on the distance of neighbors $x_i$ to the target 
point $x$. The distance calculation of $d(x_i, x)$ is based on Euclidean distance 
(in two dimension for example):
$$
d(x_i, x) = \| x - x_i \|_2
$$
 
In the spatial loess fitting, however, the Euclidean distance is apparently not 
appropriate. A more reasonable and realistic distance definition is the Great-circle 
distance, which is the shortest distance between two points on the surface of a 
sphere, measured along the surface of the sphere (as opposed to a straight line 
through the sphere's interior). The distance between two points in Euclidean 
space is the length of a straight line between them, but on the sphere there are 
no straight lines. In non-Euclidean geometry, straight lines are replaced with 
geodesics. Geodesics on the sphere are the great circles (circles on the sphere 
whose centers coincide with the center of the sphere).

Through any two points on a sphere which are not directly opposite each other, 
there is a unique great circle. The two points separate the great circle into 
two arcs. The length of the shorter arc is the great-circle distance between the 
points\cite{greatcircle}.

Explicitly, let $\phi_a$, $\lambda_a$ and $\phi_b$, $\lambda_b$ be the geographical 
latitude and longitude of two points $a$ and $b$, and $\Delta \phi$, 
$\Delta \lambda$ their absolute differences; then $\Delta \sigma$ the central angle 
between them, is given by:
$$
\Delta \sigma_{a,b} = 2 \arcsin \sqrt{ \sin^2 ( \frac{\Delta \phi}{2} ) + \cos \phi_a \cdot \cos \phi_b \cdot \sin^2 ( \frac{\Delta \lambda}{2} )}
$$ 
Then the distance $d(a,b)$, i.e. the arc length, for a sphere of radius $r$ is
$$
d(a,b) = r \Delta \lambda_{a,b}
$$

\subsection{Spatial Analysis with Elevation}

For each of 576 months after 1950, we 

\subsubsection{Conditionally Parametric Model}

A nonparametric surface is conditionally
parametric if we can divide the factors up into two disjoint subsets
A and B with the following property: given the values of the factors in A,
the surface is a member of a parametric class as a function of the the factors
in B. We say that the surface is conditionally parametric in A.
It makes sense to specify a regression surface to be conditionally parametric
in one or more variables if exploration of the data or a priori information
suggests that the underlying pattern of the data is globally a very smooth
function of the variables. Making such a specification when it is valid can
result in a more parsimonious fit.
An exceedingly simple modification of loess fitting yields a conditionally
parametric surface. We simply ignore the conditionally parametric factors
in computing the Euclidean distances that are used in the definition of the
neighborhood weights, $W_i$(x).

The method for making a loess fit conditionally parametric in a proper subset of
predictors is simple. The subset is ignored in computing the Euclidean distances 
that are used in the definition of the neighborhood weights, $\omega_i(x)$. Let 
us use an example to explain why this produces a conditionally parametric surface. 
Suppose that there are two predictors, $\mu$ and $\nu$, and $\lambda=2$. Suppose 
we specify $\mu$ to be a conditionally parametric predictor. Since the weight 
function ignores the variable $\mu$ the $i$th weight, $\omega_i(\mu, \nu)$ for 
the fit at $(\mu, \nu)$, is the same as the $i$th weight, $\omega_i(\mu+t, \nu)$, 
for the fit at $(\mu+t, \nu)$. Thus the quadratic polynomial that is fitted locally 
for the fit at $(\mu, \nu)$ is the same as the quadratic polynomial that is fitted 
locally for the fit at $(\mu+t, \nu)$, whatever the value of $t$. So for the fixed 
value of $\nu$, the surface is exactly this quadratic as a function of the first 
predictor.

\subsection{Database of Division by Station}
\label{sec:a1950.divibyStation}

Similar with section~\ref{sec:divibyStation}, we construct a database, from the
initial database, of division by station for the data after 1950. Next we 
describe the details about the procedure of creating this station based database.

\begin{description}
  \item[Input] 10,042,500 key-value pairs from the initial database, with one of
  unique 8,125 \texttt{station.id} and one of 1,236 months together as the key, 
  and a one-row R data.frame as the value. 
  \item[Output] division by station in form of 7,738 key-value pairs, with 
  \texttt{station.id} as the key, and R data.frame containing 576 observations 
  of corresponding station as the value.
  \item[Map]First input key-value pairs whose key contains year larger than 1950
  are filtered out. Next we change the key from a vector of \texttt{station.id} 
  and month index to \texttt{station.id} only. 
  \item[Reduce] 576 of one-row data.frame corresponding to one station are 
  aggregated to be one data.frame. Concretely all one-row data.frame who share
  the same \texttt{station.id} are shuffled and transferred to one reducer, and 
  then R function \texttt{rbind} is called in that reducer to combine those one-row 
  data.frame, which includes (1) \texttt{year}, year of the observation, 
  (2) \texttt{month}, month of year (January to December), (3) \texttt{tmax}, 
  the monthly maximum temperature, (4) \texttt{latitude}, (5) \texttt{longitude}, 
  and (6) \texttt{elevation}. We filter out stations who does not have any valid
  observation in 576 months. So finally there are 7,738 key-value pairs have been
  generated and save on HDFS. 
\end{description}

\subsubsection{Location of Stations}

Figure \href{../plots/a1950stations.pdf}{\ref*{a1950.location}} illustrates the 
location of 7,738 stations on the map of United States.

\begin{framed}
\begin{center}
  \href{../plots/a1950stations.pdf}{Link to figure}
  \captionof{figure}{The location of stations after 1950}
  \label{a1950.location}
\end{center}
\end{framed}

As shown in Figure \href{../plots/vertices.a1950.pdf}{\ref*{a1950.vertices}}
A kd-tree with 128 cells is built based on 7,738 stations. Then one station is 
sampled from each cell randomly. The blue dots on the graph show the locations 
of these 128 sampled stations. The red dashed lines indicate the boundary of cells 
of kd-tree. An index number from 1 to 128 is assigned to each station to 
illustrate which cell the station comes from. 

\begin{framed}
\begin{center}
  \href{../plots/vertices.a1950.pdf}{Link to figure}
  \captionof{figure}{The location of 128 stations sampled from each cell of a kd-tree}
  \label{a1950.vertices}
\end{center}
\end{framed}

We also graph the quantile plot of the log base 2 of elevation of stations in each
cell in Figure \href{../plots/elev.dist.bycell.pdf}{\ref*{elev.dist.bycell}}

\begin{framed}
\begin{center}
  \href{../plots/elev.dist.bycell.pdf}{Link to figure}
  \captionof{figure}{The quantiles of elevation of stations conditional on cell}
  \label{elev.dist.bycell}
\end{center}
\end{framed}

Later on these 128 stations are used in the section XXX for visualization and 
diagnostic purpose.

\subsection{Experiment of Tunning Parameters of STL}

In the experiment, we are trying to use consecutive 270 observations as training 
data to predict oncoming 36 months of maximum temperature. Then the prediction 
error is calculated to measure the prediction ability. 
We want to see how the different parameter setting affect 
the prediction ability. Concretely, for each station, we use the first 270 
observations to predict 36 oncoming observations, then move the range window of 
training dataset one observation ahead, and predict the next 36 observations. 
There are 576 observations for each station, so we conduct 271 prediction of 36
observations for each station.

\subsubsection{Generating Database for Experiment}

If we conduct the experiment above in series, we have to run 
$271 \times 7738 = 2096998$ number of STL fitting for all stations in R, which is 
extremely computational inefficient. Fortunately, because of using RHIPE
(R and Hadoop Integrated Programming Environment), we can easily handle this 
experiment in parallel. Intuitively, we carry out the STL fittings of multiple 
stations in parallel. Concretely, based on database of division by station, map
function which executes 271 STL prediction is applied to each of 7,738 station.
However, this is still computationally heavy for each mapper since all 271 STL 
fitting have to be sequentially executed in R with either \texttt{for} loop or 
\texttt{lapply} function. We have to consider other more efficient parallel 
procedure for the experiment.

There are other two ways to carry out the parallel procedure, which are less 
heavy with respect to computation of each mapper. One is to create a share
R object, namely a data.frame, which containing all 576 observations of all 7,738 
stations, and is saved on HDFS immediately right after the MapReduce job is 
initialized. The data.frame is order by \texttt{station.id}, \texttt{year}, and
\texttt{month}. Then in the MapReduce job, we simulate the key of input key-value
pairs to be sequence of integer from 1 to 2,096,998, each of which represents the
row index of the first observation of each STL fitting. The corresponding value 
is the same integer as the key. For instance, if key-value pair is 
\texttt{(3001,3001)}, it means the corresponding map function applied to this 
key-value pair will use observations from August 1951 to Jan 1974 (20th observation
to 289th observation inclusively) to predict the maximum temperature of Feb 1974
to Jan 1977 for the 12th station.

But this method requires each mapper which is executing map function to load the
shared R data.frame into memory of the worker on which the mapper is running. So
if one worker of the MapReduce job is assigned with multiple mappers, that shared
R object is loaded into memory with multiple copies which is inefficient with
respect to memory usage. As a result, we consider another way to perform the 
parallel computation. 

So the database we finally generated is based on 
by station database of stations after 1950. We split one key-value pairs from by 
stations database to multiple key-value pairs in map function which we called 
FlatMap function. Specifically, the map function read in one key-value pairs, one
for each STL prediction fit. Totally, there are $2,096,998$ new key-value pairs 
saved on HDFS as the database for the experiments.

We detail the procedure of generating experiment database in the following 
MapReduce job.

\begin{description}
  \item[Input] 7,738 key-value pairs from the division by station database, with 
  \texttt{station.id} as the key, and R data.frame including 576 observations as
  the value.
  \item[Output] experiment database in form of 2,096,998 key-value pairs, with 
  vector of \texttt{station.id} and month index \texttt{i} of starting date of 
  training data as the key, and R data.frame containing 271 training data 
  observations and 36 testing observations as the value.
  \item[Map] For each input key-value pairs, we create 271 intermediate key-value
  pairs. We loop over month index \texttt{i} from 1 to 271, which represents the
  starting month of the training data. The key of intermediate key-value pairs
  is vector of \texttt{station.id} and \texttt{i}, the value contains 271 
  training data (starting from \texttt{i}th month) and 36 testing data, which is 
  a R data.frame with 307 rows and 7 columns.
  \item[Reduce] Identity reduce function do not do anything to the intermediate 
  key-value pairs besides evenly distributes all intermediate key-value pairs to 
  multiple files, and save them on HDFS.
\end{description} 

Next, the parameters for STL fitting is needed to be chose and passed 
to each STL fitting. Moreover, we want to find the best parameter set based on
accuracy measurement. The purposes of choosing an accuracy measure of the 
prediction are to quantify 
the extent of uncertainty in the forecasting and in turn help selecting the best 
model based on prediction performance. 
There are many measurements which all have their strength and weakness. 
Research indicates that the performances of different methods are related to the
purpose of forecasting and the specific concerns of the situation using the 
forecasts. 
That is why from a theoretical point of view there is no single best method 
\cite{brockwell2002introduction}.



$$
Error = | y_i - \hat y_i |
$$
where $y_i$ is the observation at $i$th month, and $\hat y_i$ is prediction value
which is equal to the summation of seasonal and trend components at $i$th month.

The critiera to judge the different measurements refer to the reliability and
discrimination of a method. Reliability is defined as the capability of producing 
consistent results when the methods are applied to different subsamples of the 
same series. Discrimination is the ability to tell apart better or worse models. 
Since it is not possible to optimize these criteria at the same time, trade-off 
should be considered.

Our proposed criteria to measure the performances of various parameter sets are 
based on the absolute prediction error: E t M = |M t −P t M |. The usual choice: 
Mean Squared Error is not suitable in this case because it is influenced by the 
outliers significantly. Our series are unique in that there is fair share of 
outliers. MSE gives too much sway to the outliers and the entire loss function is
dominated by the extreme outliers in this dataset. The regular positions in the 
pattern contribute to the loss function only negligibly. What we want is a robust
solution. The absolute prediction error does not discriminate against the regular
values and limits the influence of the extreme outliers, which provides robustness 
in the assessment of the performance. On the other hand, all of the outliers are 
on the lower end of the data range. The absolute value of the error makes sure 
the influence from positive and negative errors are treated equally.

The factors we are going to tun are $w_t$, $w_s$, $d_t$, 
and $d_s$. Within each factorial experiment, we conduct multiple STL fitting with 
different parameter set for some given factors, and we define the group index of 
parameter setting to be variable \texttt{group}. For example, in the Experiment 1,
the factors we choose is $w_s$ and $w_t$, each of which has three levels. The 
total number of group of parameter setting is 9.

Next, we demonstrate the MapReduce job reading in above database from HDFS, and 
then process the STL fitting on each key-value pairs with one group of pre-defined 
parameters. 

\begin{description}
  \item[Input] 2,096,998 key-value pairs read from experiment database on HDFS.
  Key is vector of \texttt{station.id} and month index \texttt{i} of starting date
  of training data, and value is a data.frame including training data of 270 
  observations and testing data of the next 36 observations.
  \item[Output] Totally there are 7,738 multiplies total number of \texttt{group} 
  key-value pairs. 
  Key is vector of \texttt{station.id} and \texttt{group}; value is a R data.frame
  with $36 \times 271 = 9,756$ rows and 9 columns, which are (1) \texttt{year},
  (2) \texttt{month}, month of year, (3) \texttt{time}, the month index, (4) 
  \texttt{seasonal}, (5)\texttt{trend}, (6) \texttt{remainder}, (7) \texttt{tmax},
  (8) \texttt{lag}, prediction lag distance from 1 to 36, and (9) \texttt{rep},
  the replicates index of STL fitting, values from 1 to 271. 
  \item[Map] For each input key-value pairs, first we extend 270 training 
  observations with 36 \texttt{NA}s. And then pass this time series of 306 
  observations into R function \texttt{stl2} with pre-defined group of STL 
  parameter setting. Lastly a data.frame with 36 rows and 9 columns is generated
  as intermediate value, which includes the raw observations of the testing data, 
  as well as the prediction of \texttt{trend} and \texttt{seasonal} component of 
  the 36 months.
  \item[Reduce] Intermediate key-value pairs generated from map step will be saved
  on HDFS.
\end{description} 

In our design, we have four parameters in the parameter space: $w_t$, $w_s$, 
$d_t$ and $d_s$. In
every experiment, we fix the value of two factors and do a full factorial design
for anther two to explore and cover the design space. Besides, the number of inner
iterations and the number of outer iterations are also kept constant.

In the next several section, we detail the experiment set up, visualization for 
the diagnostic, and the parameter selection based on prediction error.

\subsubsection{Experiment 1,  
\textmd{$w_s=c(21, 30, 39)$, $w_t=c(231, 313, 451)$, $d_s=1$, $d_t=2$}
}

In the experiment 1, we vary the span window for seasonal and trend components. 
After the MapReduce job described previously, we first illustrate the distribution
of prediction residuals of each station for a given group of parameter setting
conditional on lag distance. Recall, in section~\ref{sec:a1950.divibyStation} we
randomly sampled 128 stations based on a kd-tree built on 7,738 stations. So we 
include these 128 stations in the graph for diagnostic purpose. Notice that there
are 1,152 ($128 \times 9$) pages of plots if one station is graphed one one page.
We describe the process of creating these graphs in parallel by using RHIPE in R
with \texttt{lattice} package.

\begin{description}
  \item[Input] $7,738 \times 9 = 69,642$ key-value pairs 
  \item[Output] 9 PDF graph files, each of which is the Normal quantile plots of
  prediction residual conditional on lag distance for 128 stations for a given
  group parameter setting.  
  \item[Map] For each of input key-value pairs, we modify the key to be 
  \texttt{group} index only, which represent which parameter setting the STL fitting
  is based on. The value data.frame is added a new column named \texttt{station.id}.
  \item[Reduce] Intermediate key-value pairs, generated by map function, which 
  shares the same \texttt{group} index are shuffled and transfered into one reducer,
  and then are further aggregated into one data.frame by calling \texttt{rbind} 
  function in R. Then graphing function from \texttt{lattice} R package is utilized
  to generate PDF graph files based on each final key-value pair. However the 
  final key-value pairs are not saved on HDFS, but only the PDF files are save on
  HDFS. 
\end{description} 

The 9 PDF files are then copied from HDFS to the local file system to be visualized
as in Figure {\ref{QQ.error.laggroup.E1}}

\begin{framed}
\begin{center}
  \href{../plots/a1950/E1/QQ.error.tmax.group.1.pdf}{Link to group 1} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E1/QQ.error.tmax.group.2.pdf}{Link to group 2} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E1/QQ.error.tmax.group.3.pdf}{Link to group 3}
\end{center}
\begin{center}
  \href{../plots/a1950/E1/QQ.error.tmax.group.4.pdf}{Link to group 4} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E1/QQ.error.tmax.group.5.pdf}{Link to group 5} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E1/QQ.error.tmax.group.6.pdf}{Link to group 6}
\end{center}
\begin{center}
  \href{../plots/a1950/E1/QQ.error.tmax.group.7.pdf}{Link to group 7} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E1/QQ.error.tmax.group.8.pdf}{Link to group 8} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E1/QQ.error.tmax.group.9.pdf}{Link to group 9}
  \captionof{figure}{The normal quantiles of prediction error conditional on lag 
  for 128 stations with a given group of parameter setting in Experiment 1}
  \label{QQ.error.laggroup.E1}
\end{center}
\end{framed}

Each station is plotted on one page, and the order of the stations for a given 
group of parameter set up is decided as following: 
based on the normal quantile plot of prediction error for each station across all 
36 lag values, the total amount of deviation of the distribution 
of prediction error from normal distribution is calculated. 
$$
$$
Then the stations are ordered from the smallest to the largest in term of deviation.


\subsubsection{Experiment 2,  
\textmd{$w_s=c(21, 41, $"periodic"$)$, $w_t=c(123, 313, 451)$, $d_s=1$, $d_t=2$}
}

\begin{framed}
\begin{center}
  \href{../plots/a1950/E2/QQ.error.tmax.group.1.pdf}{Link to group 1} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E2/QQ.error.tmax.group.2.pdf}{Link to group 2} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E2/QQ.error.tmax.group.3.pdf}{Link to group 3}
\end{center}
\begin{center}
  \href{../plots/a1950/E2/QQ.error.tmax.group.4.pdf}{Link to group 4} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E2/QQ.error.tmax.group.5.pdf}{Link to group 5} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E2/QQ.error.tmax.group.6.pdf}{Link to group 6}
\end{center}
\begin{center}
  \href{../plots/a1950/E2/QQ.error.tmax.group.7.pdf}{Link to group 7} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E2/QQ.error.tmax.group.8.pdf}{Link to group 8} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E2/QQ.error.tmax.group.9.pdf}{Link to group 9}
  \captionof{figure}{The normal quantiles of prediction error conditional on lag 
  for 128 stations with a given group of parameter setting in Experiment 2}
  \label{QQ.error.laggroup}
\end{center}
\end{framed}

\subsubsection{Experiment 3,  
\textmd{$w_s=$"periodic", $w_t=c(41, 83, 123, 313, 451)$, $d_s=1$, $d_t=c(1,2)$}
}

\begin{framed}
\begin{center}
  \href{../plots/a1950/E3/QQ.error.tmax.group.1.pdf}{Link to group 1} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E3/QQ.error.tmax.group.2.pdf}{Link to group 2} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E3/QQ.error.tmax.group.3.pdf}{Link to group 3}
\end{center}
\begin{center}
  \href{../plots/a1950/E3/QQ.error.tmax.group.4.pdf}{Link to group 4} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E3/QQ.error.tmax.group.5.pdf}{Link to group 5} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E3/QQ.error.tmax.group.6.pdf}{Link to group 6}
\end{center}
\begin{center}
  \href{../plots/a1950/E3/QQ.error.tmax.group.7.pdf}{Link to group 7} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E3/QQ.error.tmax.group.8.pdf}{Link to group 8} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E3/QQ.error.tmax.group.9.pdf}{Link to group 9}
\end{center}
\begin{center}
  \href{../plots/a1950/E3/QQ.error.tmax.group.10.pdf}{Link to group 10}  
  \captionof{figure}{The normal quantiles of prediction error conditional on lag 
  for 128 stations with a given group of parameter setting in Experiment 3}
  \label{QQ.error.laggroup}
\end{center}
\end{framed}

\subsubsection{Experiment 4,  
\textmd{$w_s=c(21, 41, $"periodic"$)$, $w_t=241$, $d_s=1$, $d_t=c(1,2)$}
}

\begin{framed}
\begin{center}
  \href{../plots/a1950/E4/QQ.error.tmax.group.1.pdf}{Link to group 1} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E4/QQ.error.tmax.group.2.pdf}{Link to group 2} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E4/QQ.error.tmax.group.3.pdf}{Link to group 3}
\end{center}
\begin{center}
  \href{../plots/a1950/E4/QQ.error.tmax.group.4.pdf}{Link to group 4} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E4/QQ.error.tmax.group.5.pdf}{Link to group 5} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E4/QQ.error.tmax.group.6.pdf}{Link to group 6}
\end{center}
\begin{center}
  \href{../plots/a1950/E4/QQ.error.tmax.group.7.pdf}{Link to group 7} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E4/QQ.error.tmax.group.8.pdf}{Link to group 8} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E4/QQ.error.tmax.group.9.pdf}{Link to group 9}
  \captionof{figure}{The normal quantiles of prediction error conditional on lag 
  for 128 stations with a given group of parameter setting in Experiment 4}
  \label{QQ.error.laggroup}
\end{center}
\end{framed}

\subsubsection{Experiment 5,  
\textmd{$w_s=c(11, 21, 31, 47$,"periodic"$)$, $w_t=241$, $d_s=c(1,2)$, $d_t=1$}
}

\begin{framed}
\begin{center}
  \href{../plots/a1950/E5/QQ.error.tmax.group.1.pdf}{Link to group 1} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E5/QQ.error.tmax.group.2.pdf}{Link to group 2} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E5/QQ.error.tmax.group.3.pdf}{Link to group 3}
\end{center}
\begin{center}
  \href{../plots/a1950/E5/QQ.error.tmax.group.4.pdf}{Link to group 4} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E5/QQ.error.tmax.group.5.pdf}{Link to group 5} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E5/QQ.error.tmax.group.6.pdf}{Link to group 6}
\end{center}
\begin{center}
  \href{../plots/a1950/E5/QQ.error.tmax.group.7.pdf}{Link to group 7} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E5/QQ.error.tmax.group.8.pdf}{Link to group 8} 
  \;\;\;\;\;\;\;\;\;\;
  \href{../plots/a1950/E5/QQ.error.tmax.group.9.pdf}{Link to group 9}
\end{center}
\begin{center}
  \href{../plots/a1950/E5/QQ.error.tmax.group.10.pdf}{Link to group 10}  
  \captionof{figure}{The normal quantiles of prediction error conditional on lag 
  for 128 stations with a given group of parameter setting in Experiment 5}
  \label{QQ.error.laggroup}
\end{center}
\end{framed}

\section{Modeling: Decomposition of Spatial Temporal Data}

Finally we shift our gears to the analysis of spatial and temporal local fitting 
of loess. 

\subsection{Time Decomposition}

\subsection{Spatial Decomposition}

\subsection{Backfitting}