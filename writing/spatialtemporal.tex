\chapter[DIVIDE AND RECOMBINED ANALYSIS TO SPATIAL-TEMPROAL \\ DATA]{DIVIDE AND RECOMBINED ANALYSIS TO SPATIAL-TEMPROAL DATA}

\section{Divide and Recombine (D\&R) for Large Complex Data}

\subsection{D\&R Statistical Framework}

D\&R \cite{Guha:2012} is a statistical framework for the analysis of large complex
data that enables feasible and practical analysis of large complex data. 

The 
analyst selects a division method to divide the data into subsets, applies an 
analytic method of the analysis to each subset independently with no communication
among subsets, selects a recombination method that is applied to the outputs 
across subsets to form a result of the analytic method for the entire data.

Analytic methods have two distinct categories, visualization methods whose outputs
are visual displays, and number-category methods whose outputs are numeric and 
categorical values. In D\&R, number-category analytic methods are typically applied
to each of the subsets. Visualization methods are typically applied to each subset 
in a sample of subsets because often there are too many of them to look at plots 
of all \cite{Hafen:2013}.
 
\subsection{Computational Environment}

The front end of our computational environment is R \cite{R}, a widely used 
software environment for statistical computing and graphics. On the other side, the
back end is the Hadoop which consists of Hadoop Distributed File 
System (HDFS) \cite{HDFS} for storage and processing engine (MapReduce) 
\cite{mapreduce}. RHIPE \cite{Guha:2010}, the R and Hadoop Integrated Programming 
Environment, bridges the gap between these two ends. 

As analyst, we only need to specify R commands to carry out a D\&R job which
consists of following three steps:
\begin{itemize}
\item Divide the whole dataset into subsets. It can be randomly dividing or
can be done conditional on a given categorical variable. For example, for a spatial
temporal data set, we can either divide the whole data by time or by location. 
\item Apply the analytic method to each subset parallelly.   
\item recombine the outputs of the A computations and write results to the HDFS. 
\end{itemize}  

The first step can be achieved by one MapReduce job using RHIPE R commands which 
creates the subsets from the original raw data set sat on HDFS and 
distributes the subsets across the servers of the cluster onto the Hadoop 
Distributed File System (HDFS) as key-value pairs. Most of situation, the original
raw data can be a raw text file saved on HDFS.

Thereafter, the second and
third steps can be implemented with another MapReduce job also specified by
analyst in RHIPE R commands. In this second MapReduce job, a group of Map
computation procedures are running embarrassingly parallel with a free core 
assigned to each, which means independently with no communication among those Map
procedures. We call those Map procedures Mapper. Then, a data block or a 
collection of subsets will be passed into those Mappers, and each subset
will be applied with the analytic method independently.


\section{Source of Data}

Historical records of weather such as monthly precipitation and temperatures from the last
century are an invaluable database to study changes and variability in climate. These data
also provide the starting point for understanding and modeling the relationship among climate,
ecological processes and human activities. 

Because of digitization and rapid development of computer hardware, nowadays
people are able to store, summarize and analyze larger and more 
complex climate data set than before, which stimulates the dramatically 
increasing  of the demand and interest of high-value environmental data and 
information. In United States, National Centers for Environmental Information, 
formerly the National Climatic Data Center (NCDC), is responsible for hosing and 
providing access to one of the most significant archives of environmental 
information. Some of data set hosted by NCEI \cite{NCEI} are shown as follows:

\begin{itemize}
  \item COOP:
    Through the National Weather Service (NWS) Cooperative Observer Program 
    (COOP), more than 10,000 volunteers take daily weather observations at 
    National Parks, seashores, mountaintops, and farms as well as in urban and 
    suburban areas. COOP data usually consist of daily maximum and minimum 
    temperatures \cite{COOP}.
  \item SNOTEL:
    The Natural Resources Conservation Service (NRCS) operates and maintains an 
    extensive and automated system (SNOwpack TELemetry or SNOTEL) designed to
    collect snowpack and related climatic data like air temperature in the Western
    United States begins in 1978 \cite{SNOTEL}.
  \item AG:
    Agricultural climate data for southeastern Washington from United States
    Department of Agriculture Natural Resources Conservation Service (USDA-NRCS) 
    \cite{USDA}.
  \item MRCC:
    Midwest Climate Data Center data, mainly for period between 1895 and 1948. 
    The MRCC \cite{MRCC} serves the nine-state Midwest region (Illinois, Indiana, 
    Iowa, Kentucky, Michigan, Minnesota, Missouri, Ohio, and Wisconsin).
  \item USHCN:
    Historical Climate Network data provides summary of the month temperature and 
    precipitation observations for 1,218 stations across the contiguous United 
    States. Temperature observations have been homogeneity corrected to remove 
    biases associated with non-climatic influences, such as changes in 
    instrumentation and observing practices, and changes to the environment 
    including station relocations \cite{USHCN}.
\end{itemize}

In 2002, based on the data set we listed above, the NCEI has further
processed them to produce a consolidated and uniform 103-year spatial temporal
data set by combining stations at similar locations, eliminating stations with
short records, and aggregating some of daily measurement to be monthly.
In summary, the data we are going to analyze is about observed monthly average
maximum daily temperatures for the conterminous US from 1895 to 1997. There are
8,125 stations reporting monthly average maximum daily temperatures at some time
in this period which includes 1,236 months. 

\section{Initial Analysis: Data and Statistics}

The FTP distribution for the 103-year monthly maximum temperature data set along
with supporting meta-data can be found at \cite{data}. 
However, the data set not only includes the raw observed monthly
maximum temperature, but also infills all missing values by statistical infilled
method conducted by IMAGe. 
When using the complete data for further statistical analysis care should be taken
with the infilled values. Although they are good estimates of the mean they may 
not reproduce the variability that one would expect from actual point 
observations of the meteorology. In statistical language, the infilled values are
the mean of the conditional distribution for the measurement given the observed 
data. They are not samples from this conditional distribution. 
So we eliminate all infilled values from our data analysis.

In this section, we describe the procedures of our data downloading and initial 
processing, followed by summary statistics of the observed monthly maximum 
temperature. Further processing of the data into multiple databases of various 
structures are discussed in details in the following sections along with the 
corresponding analyses enabled and facilitated by each database.

\subsection{Data Collection and Initial Processing}

The first step of our data collection is to parallelly download 103 data files 
from IMAGe data base. 
The raw data set is in text format with 103 files having file names in the form: 
\texttt{tmax.complete.Ynnn} with \texttt{nnn = 001, 002,} $\cdots$\texttt{, 103} 
which \texttt{001 = 1895} and \texttt{103 = 1997}. Each separate data file consists
of the maximum temperature for a single year. Each line of the file is observations
for one station according to the format: \texttt{station id}, 12 temperature 
observation (\texttt{Jan - Dec}), 12 observed/infill value codes 
(\texttt{1=missing}, \texttt{0=observed}). We only keep the observed monthly
maximum temperature in our analysis. Totally there are 8,125 stations in each file,
and temperature appears as a integer in tenths of degree

The total size of the data set is 678MB.

\subsection{Summary Statistics}
some initial plots and summary

I will look at the data trends and show how they are cyclical in nature and show little long time trends.

\begin{myframe}[width=\textwidth, bottom=28pt, top=20pt]
I certify that the work presented in the dissertation is my own unless referenced.

WHatever it is!

link to the plot
\end{myframe}

\section{Division by Station}


\section{Division by Month}

\subsection{Conditionally Parametric Model}

A nonparametric surface is conditionally
parametric if we can divide the factors up into two disjoint subsets
A and ? with the following property: given the values of the factors in A,
the surface is a member of a parametric class as a function of the the factors
in B. We say that the surface is conditionally parametric in A.
It makes sense to specify a regression surface to be conditionally parametric
in one or more variables if exploration of the data or a priori information
suggests that the underlying pattern of the data is globally a very smooth
function of the variables. Making such a specification when it is valid can
result in a more parsimonious fit.
An exceedingly simple modification of loess fitting yields a conditionally
parametric surface. We simply ignore the conditionally parametric factors
in computing the Euclidean distances that are used in the definition of the
neighborhood weights, Wi(x).

The method for making a loess fit conditionally parametric in a proper subset of predictors is simple.
The subset is ignored in computing the Euclidean distances that are used in the definition of the
neighborhood weights, $\omega_i(x)$. Let us use an example to explain why this produces a conditionally 
parametric surface. Suppose that there are two predictors, $\mu$ and $\nu$, and $\lambda=2$. Suppose
we specify $\mu$ to be a conditionally parametric predictor. Since the weight function ignores the 
variable $\mu$ the $i$th weight, $\omega_i(\mu, \nu)$ for the fit at $(\mu, \nu)$, is the same as the
$i$th weight, $\omega_i(\mu+t, \nu)$, for the fit at $(\mu+t, \nu)$. Thus the quadratic polynomial 
that is fitted locally for the fit at $(\mu, \nu)$ is the same as the quadratic polynomial that is 
fitted locally for the fit at $(\mu+t, \nu)$, whatever the value of $t$. So for the fixed value
of $\nu$, the surface is exactly this quadratic as a function of the first predictor.

\section{Modeling: }