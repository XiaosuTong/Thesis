\chapter[DIVIDE AND RECOMBINED ANALYSIS TO SPATIAL-TEMPROAL \\ DATA]{DIVIDE AND RECOMBINED ANALYSIS TO SPATIAL-TEMPROAL DATA}

\section{Source of the Data}

Historical records of weather such as monthly precipitation and temperatures from the last
century are an invaluable database to study changes and variability in climate. These data
also provide the starting point for understanding and modeling the relationship among climate,
ecological processes and human activities. 

Because of digitization and rapid development of computer hardware, nowadays
people are able to store, summarize and analyze larger and more 
complex climate data set than before, which stimulates the dramatically 
increasing  of the demand and interest of high-value environmental data and 
information. In United States, National Centers for Environmental Information, 
formerly the National Climatic Data Center (NCDC), is responsible for hosing and 
providing access to one of the most significant archives of environmental 
information. Some of data set hosted by NCEI \cite{NCEI} are shown as follows:

\begin{itemize}
  \item COOP:
    Through the National Weather Service (NWS) Cooperative Observer Program 
    (COOP), more than 10,000 volunteers take daily weather observations at 
    National Parks, seashores, mountaintops, and farms as well as in urban and 
    suburban areas. COOP data usually consist of daily maximum and minimum 
    temperatures \cite{COOP}.
  \item SNOTEL:
    The Natural Resources Conservation Service (NRCS) operates and maintains an 
    extensive and automated system (SNOwpack TELemetry or SNOTEL) designed to
    collect snowpack and related climatic data like air temperature in the Western
    United States begins in 1978 \cite{SNOTEL}.
  \item AG:
    Agricultural climate data for southeastern Washington from United States
    Department of Agriculture Natural Resources Conservation Service (USDA-NRCS) 
    \cite{USDA}.
  \item MRCC:
    Midwest Climate Data Center data, mainly for period between 1895 and 1948. 
    The MRCC \cite{MRCC} serves the nine-state Midwest region (Illinois, Indiana, 
    Iowa, Kentucky, Michigan, Minnesota, Missouri, Ohio, and Wisconsin).
  \item USHCN:
    Historical Climate Network data provides summary of the month temperature and 
    precipitation observations for 1,218 stations across the contiguous United 
    States. Temperature observations have been homogeneity corrected to remove 
    biases associated with non-climatic influences, such as changes in 
    instrumentation and observing practices, and changes to the environment 
    including station relocations \cite{USHCN}.
\end{itemize}

In 2002, based on the data set we listed above, the NCEI has further
processed them to produce a consolidated and uniform 103-year spatial temporal
data set by combining stations at similar locations, eliminating stations with
short records, and aggregating some of daily measurement to be monthly.
In summary, the data we are going to analyze is about observed monthly average
maximum daily temperatures for the conterminous US from 1895 to 1997. There are
8,125 stations reporting monthly average maximum daily temperatures at some time
in this period which includes 1,236 months. 

\section{Exploratory Analysis}

The FTP distribution for the 103-year monthly maximum temperature data set along
with supporting meta-data can be found at \cite{data}. 
However, the data set not only includes the raw observed monthly
maximum temperature, but also infills all missing values by statistical infilled
method conducted by IMAGe. 
When using the complete data for further statistical analysis care should be taken
with the infilled values. Although they are good estimates of the mean they may 
not reproduce the variability that one would expect from actual point 
observations of the meteorology. In statistical language, the infilled values are
the mean of the conditional distribution for the measurement given the observed 
data. They are not samples from this conditional distribution. 
So we eliminate all infilled values from our data analysis.

In this section, we describe the procedures of our data downloading and initial 
processing, followed by summary statistics of the observed monthly maximum 
temperature. Further processing of the data into multiple databases of various 
structures are discussed in details in the following sections along with the 
corresponding analyses enabled and facilitated by each database.

\subsection{Data Download and Initial Database}
\label{sec:Download}

The raw data set is in fixed width text format with 103 files having file names in
the form: 
\texttt{tmax.complete.Ynnn} with \texttt{nnn = 001, 002,} $\cdots$\texttt{, 103} 
which \texttt{001 = 1895} and \texttt{103 = 1997}. Each separate data file consists
of the maximum temperature for a single year. Each line of the file contains 12
monthly maximum temperature observations and 12 observed/missing value codes 
(\texttt{1=missing}, \texttt{0=observed}). We only keep the observed monthly
maximum temperature in our analysis. Totally there are 8,125 stations in each file,
and temperature appears as a integer in tenths of Celsius degree. For instance, 73
in the raw text file should be interpreted as 7.3 Celsius degrees. 

Additionally, besides the 103 raw data files, there is metadata about stations' 
information which includes:
\begin{enumerate}
  \item The station id, which uniquely identifies each station.
  \item The degree of longitude of station.
  \item The degree of latitude of station.
  \item The meter of elevation of station.
\end{enumerate}
The metadata set is in text format of 244 KB.

The first step of our 
data downloading is to parallelly download 103 data files from IMAGe data base and 
transferred from the local file system of the cluster to the HDFS. This is done
through a MapReduce job described as follow.
\begin{description}
  \item[Input] Integers from 1 to 103. There is no real input files feed into this
  MapReduce job. Instead, we simulate the key of input key-value pairs to be
  sequence of integer from 1 to 103 which is exactly the number of data set files. 
  Each of them is passed to one Mapper. 
  \item[Output] The same text files on HDFS.
  \item[Map] Download one file in each mapper. For each Mapper, the download link
  is generated by concatenating the string\\
  \texttt{https://www.image.ucar.edu/pub/nychka/NCAR\_tinfill/tmax.complete.Y}
  with the integer passed into this Mapper. The corresponding data file is
  downloaded into a temporal directory on the local file system of the server on 
  which the Mapper is running, and then this local copy of file is copied 
  into HDFS after Map is finished.  
  \item[Reduce] Reduce is not needed in this MapReduce job.
\end{description}
The total raw text files saved on HDFS are around 90MB. Totally there are 836,875 
lines in text files, and each line of the raw text file corresponded to twelve
monthly maximum temperature observations for a given station in a given year
followed by their status code of observation (observed or missing) in terms of 0/1.

\subsubsection{Construction of the Initial Database}

Next, an initial database of the whole data set is created based on plain text
files. We consider to transform the raw text files to be a more
analysis friendly structure, which only has one observation per row. Concretely,
the new structure should include following fields (columns) for each observation.
We use \texttt{NA} to represent missing value of the maximum temperature for a
given month:

\begin{enumerate}
  \item The station ID, which uniquely identifies each station.
  \item The degree of longitude of the station.
  \item The degree of latitude of the station.
  \item The meter of elevation of the station.
  \item The station name.
  \item The year of the observation.
  \item The month of the observation.
  \item The observation of the maximum temperature for the given month.
\end{enumerate}

By utilizing the RHIPE, which bridges the R and Hadoop, the new data structure can
be easily achieved within one MapReduce job by specifying RHIPE commands in R.
Moreover, each observation is saved in terms of R object, specifically a data.frame
with only one row and eight columns.
We make the choice of R data.frame because it allows us to have different type of
class for each column, and also it is extremely friendly to a great many of
statistical analysis functions in R. 

The observation database is fulfilled in a MapReduce job described as
below:

\begin{description}
  \item[Input] 103 plain text files saved on HDFS, each of which consists of 8,125 
  lines. Each line includes information about 12 months maximum temperature for a 
  given station in a given year. The year information can be pulled from last three
  characters of the name of each text file, which \texttt{001 = 1895} and 
  \texttt{103 = 1997}. Lines contain 14 tab-delimited columns:
  (1) the station id, (2) to (13) the monthly maximum temperature from January to
  December, (14) a 12-digit character which indicates if 12 monthly maximum 
  temperature is observed or missing. 
  \item[Output] 10,042,500 observations, each of which is a key-value pair, with 
  8,125 unique station ID and 1,236 month index together as the key, and a one-row 
  R data.frame as value, which includes (1) 
  \texttt{station name}, the name of station, (2) \texttt{longitude}, degree of 
  longitude of the station, (3) \texttt{latitude}, the latitude of the station, (4)
  \texttt{elevation}, the meter of elevation the station, (5) \texttt{year}, year 
  of the observation, (6) \texttt{month}, month of year (January to December), 
  (7) \texttt{tmax}, the monthly maximum temperature
  \item[Map] Parse each line of the text file as a character string. For each line,
  we split the character string based on separator \texttt{\textbackslash t} by 
  calling R function \texttt{strsplit}, and then the station ID in the first field 
  is extracted out, and with the month index together are used as the key of the 
  intermediate key-value pairs. The rest of fields are saved in a R one-row 
  data.frame as the value of the intermediate key-value pairs. The intermediate 
  key-value pairs are transferred to Reduce. 
  \item[Reduce] Identity reduce function do not do anything to the intermediate 
  key-value pairs besides evenly distributes all intermediate key-value pairs to 
  multiple files, and save them on HDFS.
\end{description}

\subsection{Summary of the Data}
\label{sec:summaryData}

In summary, the dataset covers monthly maximum temperature over the period of 103
years, from Jan 1895 to Dec 1997. 8,125 stations are scattered across the 
conterminous US excluding Hawaii and Alaska. Concretely, the elevation of 
stations ranges from -59 to 3801 meters, the longitude ranges from -124.73 degree
to -67 degree, and the latitude ranges from 24.55 degree to 49 degree. 

\subsubsection{Location of Stations}

Figure \href{../plots/allstationsone.pdf}{\ref*{all.location}} illustrates the
location of all stations on the map of United States.

\begin{framed}
\begin{center}
  \href{../plots/allstationsone.pdf}{Link to figure}
  \captionof{figure}{The location of all stations}
  \label{all.location}
\end{center}
\end{framed}

Meanwhile, we equally split the stations into 8 groups based on their elevation, 
which cutting points are at 58m, 162m, 244m, 351m, 581m, 1098m, and 1647m. And 
then location of stations are demonstrated on map of United States conditional on 
the elevation.

\begin{framed}
\begin{center}
  \href{../plots/allstations.pdf}{Link to figure}
  \captionof{figure}{The location of stations conditional on elevation}
  \label{all.location.multi}
\end{center}
\end{framed}

Besides the demonstration of stations based on longitude and latitude, the 
distribution of elevation of each station is also shown in Figure 
\href{../plots/QQelevation.pdf}{\ref*{QQelevation}}.

\begin{framed}
\begin{center}
  \href{../plots/QQelevation.pdf}{Link to figure}
  \captionof{figure}{Quantiles of elevation of stations}
  \label{QQelevation}
\end{center}
\end{framed}

In Figure \href{../plots/QQelevation.pdf}{\ref*{QQelevation}}, horizontal dash 
lines are the cutting points of elevation when we split stations into groups. 
It shows that more than
60\% of stations are below 581 meters. Figure 
\href{../plots/allstations.pdf}{\ref*{all.location.multi}} tells us that those 
stations are mainly outside of the 
Pacific Coast Ranges (officially gazetted as the Pacific Mountain System in the 
United States), which includes the Rocky Mountains, Columbia Mountains, Interior
Mountains, the Interior Plateau, Sierra Nevada Mountains, the Great Basin mountain 
ranges. Also, there are 724 station are on the west coast of United States, which
is between the Pacific Ocean and the Pacific Coast Ranges.

\subsubsection{The Number of Observation}

As we mentioned in section~\ref{sec:Download}, missing values has been saved as 
\texttt{NA} in the database. We would like to reveal where and when those 
missing values turn up.
In Figure \href{../plots/obs_month.pdf}{\ref*{obs.month}}, the log base 2 number
of observation is drawn against date. The red solid reference line in the plot is 
the $\log_2(8125)$, which is the number of observation when there is no missing 
value in that given month. The number of observation increased gradually from Jan
1895 to Jan 1915. But then there was a dramatically jump in 1931. The valid
observation number arose from 1,619 on Dec 1930 to 2,953 on Jan 1931. After Jan 
1950, the count of observation stayed around $5,800$, which is 71.3\% of all
stations. 

\begin{framed}
\begin{center}
  \href{../plots/obs_month.pdf}{Link to figure}
  \captionof{figure}{The number of observations over time}
  \label{obs.month}
\end{center}
\end{framed}

With a deeper look into the database, we found January has minimum number of 
observation in 70 out of 103 years. So the observation count is plotted against
date superposed on month of year, only for January, February, and December in 
Figure \href{../plots/obs_month_byseason.pdf}{\ref*{obs.month.season}}

\begin{framed}
\begin{center}
  \href{../plots/obs_month_byseason.pdf}{Link to figure}
  \captionof{figure}{The number of observations around Jan 1982}
  \label{obs.month.season}
\end{center}
\end{framed}

There is not huge difference between 12 month of year with respect to the 
observation count before 1981. However, Figure 
\href{../plots/obs_month.pdf}{\ref*{obs.month}} and 
Figure \href{../plots/obs_month_byseason.pdf}{\ref*{obs.month.season}} both 
indicate there is a significant drop of observation count on Jan 1982. And among 
the 16 years after that, January always has the lowest observation count.

Moreover, we found that this drop on Jan 1982 is not caused by adjustment of 
the stations such as one station shift its location to its neighborhood. It is
because the 688 stations got missing observation on Jan 1982, but then they
went back to be active again on Feb 1982, as shown in Figure 
\href{../plots/obs_Jan1982.pdf}{\ref*{Jan1982}}. The blue circles represent 
stations whose observation status were changed to be missing from December 1981
to January 1982. Meanwhile the magenta solid points represent stations whose
observation status shifted back to be valid from January 1982 to February 1982.
If a location has both blue circle and magenta solid point, it means that location
of weather station suddenly was failing on January 1982 but then went back on
February 1982. 

\begin{framed}
\begin{center}
  \href{../plots/obs_Jan1982.pdf}{Link to figure}
  \captionof{figure}{The location of active stations around Jan 1982}
  \label{Jan1982}
\end{center}
\end{framed}

We believe this astonishing number of failure of stations on January 1982 is caused
by severe weather on January 1982, which may cause the battery in the weather 
station died. As shown in Figure \href{../plots/monthmean.pdf}{\ref*{min.monthmean}},
the mean of monthly minimum temperature over all stations are plotted against 
date from 1931 to 1998. We found on January 1982, it reached the minimum value 
of $-15.65$. And this is only for stations who has record in that month, there 
were a lot of stations had missing value may have even lower records.  
From January 11 to January 17, A brutal cold snap sends temperatures to all-time 
record lows in dozens of cities throughout the Midwestern United States. Especially
on January 17, 1982, so called "Cold Sunday"\cite{coldsunday}, in numerous cities 
temperatures fall to their lowest levels in over 100 years.

\begin{framed}
\begin{center}
  \href{../plots/monthmean.pdf}{Link to figure}
  \captionof{figure}{The monthly minimum temperature vs. date}
  \label{min.monthmean}
\end{center}
\end{framed}

\section{Monthly Maximum Temperature without Missing Value}

Before we jump into the analysis of spatial and temporal dimension interactively, 
we start our analysis with the time series analysis on each station independently.
In this section we consider the stations without missing value, which
there are 432 stations in total. The for visualization purpose, we randomly sample
100 stations out of them. Their locations are shown as in Figure 
\href{../plots/100stations.pdf}{\ref*{100stations}}. For each station, we use 
STL (Seasonal Trend Loess) method to analysis the time series. 
Additionally, we would like to carry out the analysis for each station in parallel. 

\begin{framed}
\begin{center}
  \href{../plots/100stations.pdf}{Link to figure}
  \captionof{figure}{The location of the 100 stations}
  \label{100stations}
\end{center}
\end{framed} 

\subsection{Division by Station}
\label{sec:divibyStation}

In order to analyze multiple time series in parallel, we need a database or division,
which groups observations of one station all together. And then STL method is 
applied to each station independently. Finally, the results of decomposition from
STL method needs to be readily presented by visualization method so that we can
compare the results cross multiple stations. 

Fortunately, by using RHIPE, which integrates the R and Hadoop, the division by 
station and the STL analysis on each station can be handily accomplished.
On the one hand, R provides analysis friendly data structure and powerful 
statistical analysis and visualization packages such as STL+, lattice package.
On the other hand, Hadoop enables the efficient processing of divisions, such as
subsets of division by station can be represented as key-value pairs saved on 
HDFS. In the following, we describe in details the procedures to construct the 
division by station database via RHIPE on a cluster of 11 servers and 242 processors.

\begin{description}
  \item[Input] 10,042,500 key-value pairs from the initial database, with 8,125 
  unique \texttt{station.id} and 1,236 months as the key, and a one-row R data.frame 
  as the value. The value includes (1) \texttt{station.name}, the name of station, (2) 
  \texttt{longitude}, degree of longitude of the station, (3) \texttt{latitude}, 
  the latitude of the station, (4) \texttt{elevation}, the meter of elevation the 
  station, (5) \texttt{year}, year of the observation, (6) \texttt{month}, month 
  of year (January to December), (7) \texttt{tmax}, the monthly maximum temperature.
  \item[Output] division by station in form of 432 key-value pairs, with 
  \texttt{station.id} as the key, and R data.frame containing 1,236 observations
  information of corresponding station as the value.
  \item[Map]For every input key-value pair, we do not carry out any adjustment on
  the one-row data.frame of value. But we do change the key from a vector of 
  \texttt{station.id} and month index to \texttt{station.id} only. 
  \item[Reduce] 1,236 of one-row data.frame corresponding to one station are 
  aggregated to be one data.frame. Concretely all one-row data.frame who share
  the same station ID are shuffled and transferred to one reducer, and then R function 
  "rbind" is called in that reducer to combine those one-row data.frame, which includes
  (1) \texttt{year}, year of the observation, (2) \texttt{month}, month of year 
  (January to December), (3) \texttt{tmax}, the monthly maximum temperature. The 
  rest of information about the station, \texttt{station.name}, \texttt{latitude},
  \texttt{longitude}, \texttt{elevation}, is save as an attributes associated with the
  data.frame named \texttt{location}.
\end{description}

\subsection{Time Series Analysis of Each Subset}

After we created the database of division by station, we are ready for the parallel 
analysis of all subsets of the database. The analysis method we apply to each station
is Seasonal Trend Loess (SLT). Specifically, we utilize R package named stl2\cite{stl2},
which provides several enhancements including the ability to deal with missing values 
and higher order polynomial smoothing compared to the stl method that ships with 
base R. Then we collect the analysis result of 100 randomly sampled stations together 
to carry out the visualization. We detail the process of corresponding map-reduce 
job below.

\begin{description}
  \item[Input] 432 key-value pairs from the division by station database, with 432 
  unique \texttt{station.id} as the key, and a R data.frame as the value. The value 
  includes (1) \texttt{year}, year of the observation, (2) \texttt{month}, month 
  of year (January to December), (3) \texttt{tmax}, the monthly maximum temperature.
  And an attributes named \texttt{location} encompassing \texttt{station.name},
  \texttt{latitude}, \texttt{longitude}, and \texttt{elevation} is attached to the
  data.frame.
  \item[Output] One key-value pair with 1 as the key, and R data.frame with 123,600
  rows and 11 columns, which contains \texttt{tmax}, seasonal components, trend 
  components, and remainders, \texttt{year}, \texttt{month}, and station location 
  information: \texttt{latitude}, \texttt{longitude}, \texttt{elevation}. 
  \item[Map]For every input key-value pair, we first order the 1,236 observations 
  of one station by month index from 1 to 1,236. Then we apply STL method on the 
  time series of maximum temperature. The fitting result is also saved in a 
  data.frame. Next, we add a new column to the data.frame which is the corresponding 
  \texttt{station.id}. Next, we filter out key-value pairs whose key 
  \texttt{station.id} is not belong in that 100 stations. Finally, all keys are 
  replaced to be 1 so that fitting results of all 100 stations can be accumulated 
  into one data.frame in the reducer. \item[Reduce] 100 intermediate key-value 
  pairs who share the same key 1 are transfered into one reducer. In the reduce 
  function, they are combined together by calling R function "rbind" to generate 
  the final one data.frame. 
\end{description}

As shown in the reduce function above, a set of predefined parameter setting for 
STL and a string vector of the 100 sampled station ID are passed into the map-reduce 
job by utilizing \texttt{parameters} argument in \texttt{rhwatch} function, which 
is used to share R objects in the Global Environment of R session on the front-end 
server to all workers of the map-reduce job. After the fitting, we would like to 
visually check the model validation, with same parameter setting, across all 
different stations.

\subsubsection{First Run}

In the first run of parallel STL fit, we specify the parameters to be: 
\texttt{t.window} is 495, \texttt{t.degree} is quadratic, \texttt{s.window} is 
77, and \texttt{s.degree} is linear, \texttt{inner} is 10. The first thing we would 
like to check about with respect to the STL fitting is comparing the fitted value 
with the raw observations. In Figure 
\href{../plots/100stations/first_run/fitted.100stations.tmax.pdf}
{\ref*{firstrun.fitted.100stations}}, each page is one station. The whole time 
series with 1,236 months for one station is chunked into 9 periods. Each of the 
first 8 periods has 144 monthly observations, the last period has 84 monthly 
observations. The raw observations are drawn with blue points, and the seasonal 
component plus trend component are drawn with magenta curve.  

\begin{framed}
\begin{center}
  \href{../plots/100stations/first_run/fitted.100stations.tmax.pdf}{Link to figure}
  \captionof{figure}{The fitted value vs. month}
  \label{firstrun.fitted.100stations}
\end{center}
\end{framed}

Clearly, no matter where the station is, the fitted values (seasonal+trend) are 
very close to the true values, and also they capture the seasonal pattern in the 
raw data. Next, we carry out the diagnostic for each components based on visualization.

\textbf{Trend Components}

In Figure \href{../plots/100stations/first_run/trend.100stations.tmax.pdf}
{\ref*{firstrun.trend.100stations}}, the trend component is drawn against to month
index in blue curve conditional on \texttt{station.id}. So each panel is about 
the trend component of one station. Meanwhile, we plot with trend components 
the moving average of yearly mean of monthly maximum temperature.
Collectively, the same STL parameter setting still can capture the different trend
behavior across all stations.

\begin{framed}
\begin{center}
  \href{../plots/100stations/first_run/trend.100stations.tmax.pdf}{Link to figure}
  \captionof{figure}{The trend components vs. month index}
  \label{firstrun.trend.100stations}
\end{center}
\end{framed}

\textbf{Seasonal Components}

The diagnostic plot for seasonal component is shown in Figure 
\href{../plots/100stations/first_run/sea+remaind.month.100stations.tmax.pdf}
{\ref*{firstrun.searemaind.100stations}}, named seasonal-diagnostic plots, which
helps us decide how much of the variation in the data other than trend component
should go into the seasonal component and how much into the remainder.

\begin{framed}
\begin{center}
  \href{../plots/100stations/first_run/sea+remaind.month.100stations.tmax.pdf}
  {Link to figure}
  \captionof{figure}{The sum of seasonal components and remainders vs. year}
  \label{firstrun.searemaind.100stations}
\end{center}
\end{framed}

Seasonal component and remainders of
one station are plotted against to year on one page conditional on month of year
(Jan to Dec). Let $\bar S_k$ be the mean of the seasonal component for the $k$-th 
month of year. The magenta curve in the panel for the $k$-th month of year is 
seasonal components subtract out their mean $\bar S_k$. The blue points are seasonal
components plus remainders for $k$-th month of year minus their mean $\bar S_k$ 
as well. The reason for subtracting $\bar S_k$ is to center the values on each 
panel at zero; note that the vertical scales of all panels of all stations are 
the same so that we can graphically compare the variation of values on different 
panels.

\textbf{Remainders}

The first figure about remainder is shown in Figure 
\href{../plots/100stations/first_run/remainder.month.100stations.tmax.pdf}
{\ref*{firstrun.remaind.100stations}}. Remainder of each station is graphed against
to year conditional on month of year. Also a loess smoothing curve with span equal
to 0.75 is plotted for each month of year. A flat loess smoothing line represents
the seasonal component does capture reasonable amount of variation in each 
sub-series.

\begin{framed}
\begin{center}
  \href{../plots/100stations/first_run/remainder.month.100stations.tmax.pdf}
  {Link to figure}
  \captionof{figure}{The remainders vs. year}
  \label{firstrun.remaind.100stations}
\end{center}
\end{framed}

\begin{framed}
\begin{center}
  \href{../plots/100stations/first_run/QQ.remainder.100stations.tmax.pdf}
  {Link to figure}
  \captionof{figure}{The normal quantiles of remainders}
  \label{firstrun.QQremaind.100stations}
\end{center}
\end{framed}

\begin{framed}
\begin{center}
  \href{../plots/100stations/first_run/remainder.acf.100stations.tmax.pdf}
  {Link to figure}
  \captionof{figure}{Autocorrelation functions for remainder term}
  \label{firstrun.remaindacf.100stations}
\end{center}
\end{framed}

As in Figure 
\href{../plots/100stations/first_run/remainder.acf.100stations.tmax.pdf}
{\ref*{firstrun.remaindacf.100stations}},
although there are a few slightly significant autocorrelation estimates for some
of stations, surprisingly the overall behavior is indicative of independence.


\subsubsection{Second Run}

In the first run of parallel STL fit, we specify that parameters to be: 
\texttt{t.window} is 495, \texttt{t.degree} is quadratic, \texttt{s.window} is 
77, and \texttt{s.degree} is linear, \texttt{inner} is 10. However we randomly 
sample another 100 stations to be in our visualization checking because we would 
like to see if parallel STL fitting is robust cross different parameter setting 
and different locations.


\section{Monthly Maximum Temperature after 1950}

In the previous section, we illustrated how to apply STL on 432 stations in parallel
with same STL parameter setting by utilizing RHIPE computational framework. In 
this coming section, we start with a demonstration of experiments of model selection,
which select the best of parameter setting of STL model cross stations. Then we
explore the spatial loess fit for a given month. 

Recall in the Figure \href{../plots/obs_month.pdf}{\ref*{obs.month}}, we notice
the number of active stations gradually stabilized after Jan 1950. Since long
period of missing observation is not the main concern of this thesis, we decide
only consider stations with observations after Jan 1950.

\subsection{Database of Division by Station}

Similar with section~\ref{sec:divibyStation}, we construct a database, from the
initial database, of division by station first for the data after 1950. Next we 
describe the details about the procedure of creating this station based database.

\begin{description}
  \item[Input] 10,042,500 key-value pairs from the initial database, with 8,125 
  unique \texttt{station.id} and 1,236 months as the key, and a one-row R 
  data.frame as the value. 
  \item[Output] division by station in form of 7,738 key-value pairs, with 
  \texttt{station.id} as the key, and R data.frame containing 576 observations 
  of corresponding station as the value.
  \item[Map]First input key-value pairs whose key contains year larger than 1950
  are filtered out. Next we change the key from a vector of \texttt{station.id} 
  and month index to \texttt{station.id} only. 
  \item[Reduce] 1,236 of one-row data.frame corresponding to one station are 
  aggregated to be one data.frame. Concretely all one-row data.frame who share
  the same station ID are shuffled and transferred to one reducer, and then R function 
  "rbind" is called in that reducer to combine those one-row data.frame, which includes
  (1) \texttt{year}, year of the observation, (2) \texttt{month}, month of year 
  (January to December), (3) \texttt{tmax}, the monthly maximum temperature. The 
  rest of information about the station, \texttt{station.name}, \texttt{latitude},
  \texttt{longitude}, \texttt{elevation}, is save as an attributes associated with the
  data.frame named \texttt{location}.
\end{description}

\subsection{Database of Division by Month}

\begin{description}
  \item[Input] 10,042,500 key-value pairs from the initial database, with 8,125 
  unique \texttt{station.id} and 1,236 months as the key, and a one-row R 
  data.frame as the value. The value includes (1) \texttt{station.name}, the name of station, (2) 
  \texttt{longitude}, degree of longitude of the station, (3) \texttt{latitude}, 
  the latitude of the station, (4) \texttt{elevation}, the meter of elevation the 
  station, (5) \texttt{year}, year of the observation, (6) \texttt{month}, month 
  of year (January to December), (7) \texttt{tmax}, the monthly maximum temperature.
  \item[Output] division by station in form of 432 key-value pairs, with station 
  ID as the key, and R data.frame containing 1,236 observations information of
  corresponding station as the value.
  \item[Map]For every input key-value pair, we do not carry out any adjustment on
  the one-row data.frame of value. But we do change the key from a vector of 
  \texttt{station.id} and month index to \texttt{station.id} only. 
  \item[Reduce] 1,236 of one-row data.frame corresponding to one station are 
  aggregated to be one data.frame. Concretely all one-row data.frame who share
  the same station ID are shuffled and transferred to one reducer, and then R function 
  "rbind" is called in that reducer to combine those one-row data.frame, which includes
  (1) \texttt{year}, year of the observation, (2) \texttt{month}, month of year 
  (January to December), (3) \texttt{tmax}, the monthly maximum temperature. The 
  rest of information about the station, \texttt{station.name}, \texttt{latitude},
  \texttt{longitude}, \texttt{elevation}, is save as an attributes associated with the
  data.frame named \texttt{location}.
\end{description}

\subsubsection{Conditionally Parametric Model}

A nonparametric surface is conditionally
parametric if we can divide the factors up into two disjoint subsets
A and ? with the following property: given the values of the factors in A,
the surface is a member of a parametric class as a function of the the factors
in B. We say that the surface is conditionally parametric in A.
It makes sense to specify a regression surface to be conditionally parametric
in one or more variables if exploration of the data or a priori information
suggests that the underlying pattern of the data is globally a very smooth
function of the variables. Making such a specification when it is valid can
result in a more parsimonious fit.
An exceedingly simple modification of loess fitting yields a conditionally
parametric surface. We simply ignore the conditionally parametric factors
in computing the Euclidean distances that are used in the definition of the
neighborhood weights, Wi(x).

The method for making a loess fit conditionally parametric in a proper subset of predictors is simple.
The subset is ignored in computing the Euclidean distances that are used in the definition of the
neighborhood weights, $\omega_i(x)$. Let us use an example to explain why this produces a conditionally 
parametric surface. Suppose that there are two predictors, $\mu$ and $\nu$, and $\lambda=2$. Suppose
we specify $\mu$ to be a conditionally parametric predictor. Since the weight function ignores the 
variable $\mu$ the $i$th weight, $\omega_i(\mu, \nu)$ for the fit at $(\mu, \nu)$, is the same as the
$i$th weight, $\omega_i(\mu+t, \nu)$, for the fit at $(\mu+t, \nu)$. Thus the quadratic polynomial 
that is fitted locally for the fit at $(\mu, \nu)$ is the same as the quadratic polynomial that is 
fitted locally for the fit at $(\mu+t, \nu)$, whatever the value of $t$. So for the fixed value
of $\nu$, the surface is exactly this quadratic as a function of the first predictor.

\section{Modeling: Decomposition of Spatial Temporal Data}

Finally we shift our gears to the analysis of spatial and temporal local fitting 
of loess. 

\subsection{Time Decomposition}

\subsection{Spatial Decomposition}

\subsection{Backfitting}