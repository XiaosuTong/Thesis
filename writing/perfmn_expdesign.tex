\section{Experiment Design}

\subsection{Hadoop User Tuning Parameters}

\subsubsection{\texttt{mapreduce.task.io.sort.mb}}

This parameter controls the size of memory buffer, in megabytes, which is used 
to hold the map output in each Mapper. The value for it is integer varies from 1 
to 2047. The default value for this parameter is 100, which only allocates 100MB
of memory from heap size of JVM for saving map output. It is quite small in 
general. For jobs like reading in and swapping, it is definitely worth to increase
the value of this parameter to give more memory for holding output of each Mapper.

\subsubsection{\texttt{mapreduce.map.sort.spill.percent}}

This parameter works with \texttt{mapreduce.task.io.sort.mb} collectively to 
control the memory used for holding map output. Concretely, a circular memory 
buffer is allocated for each mapper to write intermediate output to. When the 
map output occupied \texttt{mapreduce.map.sort.spill.percent} percent of \\
\texttt{mapreduce.task.io.sort.mb}, the output is spilled to local disk of node
where Mapper is running. Meanwhile the Mapper is keeping writing output to this
circular memory buffer when spilling is proceeding. However if the memory buffer
is filled up during this time period, the Mapper is paused until the spill is
finished. Clearly, it is critical to set this parameter as well as the total 
amount of memory buffer to be high enough if there are numerous amount of output
generated by each Mapper. However, we do not want set them to be too large.
Because the memory buffer is still belongs to the Mapper JVM heap size, which is
decided by \texttt{mapreduce.map.java.opts}. Reserving too much memory from the
total heap size of JVM will leave limited memory usage for other processes sponsed
by the JVM, and indeed will force JVM to involve more garbage clean, which in 
turn will hurt the job performance.    

\subsubsection{\texttt{mapreduce.task.io.sort.factor}}

Every time when the contents in the memory buffer specified by \\
\texttt{mapreduce.task.io.sort.mb} reaches the spill percent threshold, a new
spill file is generated. So there may be multiple output files generated after 
the Map is finished. All of these spill files are merged into one sorted and
partitioned file. This is done in in rounds, and the number of files merged in 
each round is controlled by \texttt{mapreduce.task.io.sort.factor}. The default 
value is 10.

\subsubsection{\texttt{mapreduce.job.reduce.slowstart.completedmaps}}

The parameter controls when Reducers should be launched. Specifically, it specifies
the fraction of the number of Mappers which should be complete before Reducers
are scheduled for the job. Under MapReduce2 (YARN) \cite{YARN} framework, this 
parameter becomes critical if the Map of MapReduce job is time consuming. Setting 
this parameter to be a low value as default, which is 0.05, can start the Reducers
doing nothing but waiting for the output from Mappers. However this waist several
containers assigned for Reducers without doing anything instead of assigning them
to the Mappers under pending statues.   

\subsubsection{\texttt{mapreduce.reduce.shuffle.parallelcopies}}

After the map step is finished, the intermediate key-value pairs are partitioned
and sitting on the local disk of the node where Mapper ran. Then each Reducer 
the corresponding partition from all Mapper outputs through the network. And
actually each Reducer evokes a number of copy threads, which controlled by this
parameter, to fetch the intermediate output of Mapper in parallel. This parameter
should be set wisely. Too large value of copy threads will waste for the CPU,
but too small number of copy threads will slow down the copy step of Reducer.
The default value is 5.

\subsubsection{\texttt{mapreduce.reduce.shuffle.input.buffer.percent}}

During the copying stage, the output of Map is copied to part of the total heap 
space of Reducer's JVM. This parameter controls how large the proportion is.
The default value of this parameter is 0.7. Suppose the total heap size of the
JVM of Reducer is 4GB, then 2.8 GB of JVM's memory is used for holding the output
copied from the local disk of nodes ran Mappers.

\subsubsection{\texttt{mapreduce.reduce.shuffle.merge.percent}}

Similar with the Mapper, there is also a spilling mechanism to avoid memory 
overflow. Specifically, the Map output is consistently copied to the proportion 
of heap space specified by \texttt{mapreduce.reduce.shuffle.input.buffer.percent}.
Once the contents in the memory buffer reaches threshold controlled by two 
parameters collectively, the intermediate outputs or the inputs to Reduce is 
spilled to local disk of the node which runs the Reduce. One of the two parameters 
is \\
\texttt{mapreduce.reduce.shuffle.merge.percent} which controls the threshold in 
term of percent of buffer size. If the size of buffer size is occupied over this
proportion, spilling to disk will be triggered. By default, this is set to be 
0.66. Suppose the heap size of Reduce JVM is 4 GB as well, the memory buffer for
coping is 2.8 GB by setting \texttt{mapreduce.reduce.shuffle.input.buffer.percent}
to be 0.7. Then the size threshold is 1.85 GB. Another parameter controls the
threshold is the \texttt{mapreduce.reduce.merge.inmem.threshold} which is discussed
in the following paragraph.

\subsubsection{\texttt{mapreduce.reduce.merge.inmem.threshold}}

Parameter \texttt{mapreduce.reduce.shuffle.merge.percent} controls the memory
buffer threshold in term of the size of contents. Meanwhile the parameter \\
\texttt{mapreduce.reduce.merge.inmem.threshold} controls the contents in memory
buffer in term of counts. So the intermediate output held in the memory buffer
cannot be too many or too large, otherwise they will be spilled to local disk.
The default is 1,000. By setting this parameter to be 0 will hand over the control
of spilling behaviors fully to the \texttt{mapreduce.reduce.shuffle.merge.percent}. 
Notice that the spilled files on local disk are sorted by key. Once all 
intermediate outputs are copied, the Reducers starts to merge and sort all those
spilled files into several sorted files which will be feed to the reduce function
as we defined. This merge process is done in rounds, and the number of merge files
at each round is controlled by \texttt{mapreduce.task.io.sort.factor}, similar 
as in Map stage. Until the number of merged files are equal or less than the number 
specified by \\ \texttt{mapreduce.task.io.sort.factor}, instead of merging them 
into one file, they are directly fed into reduce function. Moreover, not all
of the intermediate data in the memory buffer are spilled into disk. Portion of 
the intermediate data are reserved in the memory, and size is controlled by the
next parameter.

\subsubsection{\texttt{mapreduce.reduce.input.buffer.percent}}

This parameter is specifying the proportion of total heap size of Reducer's JVM
used to reserve the intermediate data in memory and directly feeds the reduce
function. By default, this parameter is set to be 0, which forces all intermediate
data to the local disk and leave all memory for the computation of reduce function.
However, if the memory utilization of reduce computation is extremely light, then
we can increase this parameter to be value close to 1 to keep more intermediate
data in the memory on the reduce side and save strips to the local disk, which
will improve the job performance. In \cite{li2014mronline}, it also suggests to
set this value to be same as \texttt{mapreduce.reduce.shuffle.merge.percent}. 

\subsubsection{\texttt{mapreduce.job.reduces}}

This parameter controls the number of reduce tasks, which palys a critical rule
on the performance of reduce side. One fact of our analysis routine is that it is
a sequence of MapReduce jobs. The sequence file generated from one MapReduce job
is the input to the next job. So it is crucial to decide the number of reduce
tasks because it also controls the number of files in the output path on HDFS. 
From a given MapReduce job, the number of reduce tasks should be set to be a 
reasonable number because this controls the size of intermediate data sent to 
each reducer under the assumption that each key-value pair has roughly the same
size in term of memory usage. In other words, this parameter controls the amount
of data that will be processed by each reducer. If too few of reduce tasks are
specified, then more intermediate data is sent to each reducer. Then it is more
likely to invoke the multiple trips to local disk on the reduce side as we 
mentioned in the paragraph about \texttt{mapreduce.reduce.shuffle.merge.percent}.
Once this MapReduce job finished, each reducer generated a file on HDFS hold the 
output key-value pairs. The next MapReduce job read in those files and initialize
several map tasks for each file, and the number of map tasks for each file is 
determined by
\begin{equation} 
\frac{\text{size of the file}}{\text{block size of HDFS}}
\end{equation}
The remainder of the division above will be also stored as a block on HDFS, and
it starts a map task as well even if it only has 1 MB amount of data. If too much
files are generated in the previous MapReduce job with a large number of reduce
tasks, each of which highly likely has a tiny block whose size is much smaller 
than an usual block. Those small blocks still occupies the same amount of resource
as a full block does, which is dramatically inefficient. 


\section{Software Implementation of R Package}
\label{sec:Rpackage}

\subsection{\texttt{Spaloess} Package}

\texttt{Spaloess} is a R package for spatial loess smoothing. It is highly 
depends on the original \texttt{loess} function in the \texttt{stats} package
in base R. There are two main functions in the \texttt{Spaloess} package, which 
are \texttt{spaloess} function for spatial loess smoothing, and \texttt{predloess} 
for the spatial prediction using loess smoothing. Most of implementation in these
two functions are kept the same as the \texttt{loess} and \texttt{predict.loess}
functions, which is R wrapper functions. All memory allocation are done in 
C, and real computation engine is implemented in FORTRAN. \texttt{Spaloess} does
offer the following advantages compared with original \texttt{loess}:

\begin{itemize}
\item Two different distance calculation are available. Euclidean distance and 
Great-Circle distance are allowed. For Great-Circle distance, the input spatial 
attributes must be longitude and latitude degree. Great-Circle distance 
calculation is implemented in FORTRAN.
\item Interpolation kd-tree is built based on all locations instead of only 
non-NA locations as in \texttt{loess} function. 
\item Missing values in the dataset can be handled directly within 
\texttt{spaloess}. 
\end{itemize}

In the original implementation of \texttt{loess} function, the kd-tree 
\cite{bentley1980multidimensional} for 
interpolation is only built based on observations that are not missing values. 
Those missing observations are directly excluded from the analysis. It makes 
extremely harder and computational expensive to predict at those missing value 
especially if those missing value are outside the boundary of the space spanned
by all independent variables. Instead, in the \texttt{spaloess} function, kd-tree 
is built based on all observations even those with missing value. Then 
interpolation can be easily conducted at every location including missing value.

\subsection{\texttt{drSpaceTime} Package}

\texttt{drSpaceTime} is a R package for spatial temporal analysis using divide 
and recombined concept. It is highly depends on three exist R packages: 
\texttt{Rhipe}, \texttt{stlplus}, and \texttt{Spaloess}, which are all open 
source and available on Github \cite{github}. Detailed documentation and examples
can be found in the appendix.

As demonstrated in \ref{sec:routine}, there are seven steps in the modeling 
routine of spatial-temporal data. Each of steps are implemented in a function
from \texttt{drSpaceTime} package, such as \texttt{readIn()} to read in the raw
text files and generating by time division on HDFS; \texttt{spaofit()} to produce
spatial smoothing fit of original observation at each time point; 
\texttt{swaptoLoc()} to generate by location division from the by time division 
including spatial smoothed values; \texttt{stlfit()} to carry out temporal fitting
of the spatial smoothed value in each location by calling \texttt{stlplus} 
function; \texttt{swaptoTime()} to generate by time division from the by location
division which includes all STL fitted components; \texttt{sparfit()} to 
carry out spatial smoothing fit of the remainder component of STL fit.

Besides those functions which represents each steps of the modeling routine, 
there are other two functions \texttt{spacetime.control()} and 
\texttt{mapreduce.control()}, which are both returning a R list object. Concretely,
the \texttt{mapreduce.control()} returns a list including all user tunable 
Hadoop parameters used in a given MapReduce job. \texttt{spacetime.control}, on
the other hand, returns a list with all smoothing parameters needed either for
spatial smoothing or temporal smoothing. 

Finally, the last function in the package is \texttt{predNew()}.