\chapter{BACKGROUND}

This chapter introduces local regression and some time series modeling methods,
with emphasis on a time series decomposition method: seasonal trend decomposition
using loess (STL), and a spatially local regression method. The goal is to be
brief but still cover the methods thoroughly, as much of the new work developed
later depends heavily on these topics.

\section{Nonparametric Regression}

Regression analysis traces the mean value of a response variable ($y$) as a 
function of one or several predictors ($x$'s). Suppose that there is $n$ pairs 
of observations: $(x_1, y_1), \cdots, (x_n, y_n)$, the object of regression 
analysis is to model the relationship between $y$ and $x$ in the form of
\begin{equation} 
y_i = f(x_i) + \epsilon_i
\end{equation} 
where $f$ is an unknown function, $\epsilon_i$ is the random error term in the
observation which cannot be explained by the model. 
As it is usually practiced in parametric approach, regression analysis assumes a 
known form of function for $f(x_i)$, which is fully described by a finite 
set of parameters, to be estimated. For example, linear function of predictor 
variables:
\begin{equation} 
f(x_i) = \beta_0 + \beta_1x_i
\end{equation} 
where $\beta$s are the parameters in the model to be estimated. The relationship 
between response variable and predictor variable is described and summarized 
easily by the estimated parameters.

But in real data analysis, $f(x)$ is flexible and unknown, we have to relate $y$ 
to $x$ without assuming any functional form. And this is when nonparametric 
model comes in. Concretely, nonparametric regression is a form 
of regression analysis in which the relationship between response variable and 
predictor does not have an explicit form but is constructed highly to the
information derived from the data.

\subsection{Local Regression models}

One approach in the nonparametric regression family to modeling an unknown and
complex $f$ is called local regression. Instead of assuming 
$f(x)$ to have a parametric form overall, we assume that the data can locally, 
within some neighborhood of $x_0$, be well-approximated by a parametric form which 
can be polynomial or even constant. For instance, let a neighborhood of $x_0$ be
$x \pm h$ for some bandwidth $h > 0$.
Nadaraya \cite{nadaraya1964} and Watson \cite{watson1964} proposed a local 
constant estimator, or kernel regression method, with a pre-defined kernel 
function as weighting function, called Nadaraya-Watson estimator:
\begin{equation} 
\hat f(x) = \sum_{i=1}^{n} \omega_{i}y_i
\end{equation} 
where 
\begin{equation} 
\omega_i = \frac{ K(\frac{x_i - x}{h})}{\sum_{i=1}^n K(\frac{x_i - x}{h})}
\end{equation} 
and $K(\cdot)$ is a non-negative real-valued integrable function, which satisfying 
the following two requirements
\begin{equation} 
\int_{-\infty}^{\infty} K(\mu)d\mu = 1
\end{equation} 
\begin{equation} 
K(-\mu) = K(\mu)
\end{equation}
The same idea can be illustrated when replacement of a quantitative variable by
indicator variables. It uses a constant kernel function $K(\mu) = 1$ to assign
equal weights to all neighbors within $x\pm h$. 

However, no matter which kernel function is chosen, or what the bandwidth $h$ is,
NW estimator has its limitation. It actually fits a weighted constant regression
line in the neighborhood of $x_0$. The prediction variable only effects the weights
of each neighbors of target point $x_0$. Kernel regression cannot even estimate 
a linear surface without bias \cite{cleveland1988regression}, especially on the 
boundaries of the domain. A more general model than local constant
model is to fit a low order polynomial in the neighborhood of $x_0$. So the predictor
variable not only effects the weights of neighbors, but also the local model itself.
In the next subsection, we will present more detail about local regression model
specifically the local polynomial model.

\subsection{Fitting a Local Polynomial Model}
\label{sec:loess}

There are several ways to fit a local polynomial models, we mainly focus on
particular one: $loess$. It is a more general and flexible method than the kernel
regression we discussed above. Cleveland introduced this locally weighted regression
method in \cite{Cleveland:1979}. Later on, other references about the methodology, 
theory, and computation for the loess method are well illustrated in 
\cite{cleveland1988locally}, \cite{cleveland1991computational}, 
\cite{cleveland1988regression}, and \cite{cleveland1996smoothing}. 

Suppose we are trying to fit at target point $x_0$ in the predictor space with
one dimension. The loess fit is obtained by locally weighted least square with a
polynomial of degree $\lambda$ using $n_{\alpha}$ nearest neighbors of $x_0$. 
Concretely, a smoothing parameter $\alpha$ is defined between 0 and 1, which is
the ratio of number of observations included in the model as neighbors. Then 
\begin{equation} 
n_{\alpha} = \min(n, \lfloor \alpha n \rfloor)
\end{equation}
A weight is assigned to each neighbors based on their distance to the target $x_0$.
The closer neighbors is to the $x_0$, the higher weight the neighbors will get. The
weight is defined as a function of the distance. Specifically, the tri-cube weight 
function is used.
\begin{equation} 
T(\mu) =
  \begin{cases}
    (1 - \mu^3)^3       & \quad \text{for } 0 \le \mu < 1\\
    0  & \quad \text{for } \mu \ge 1\\
  \end{cases}
\end{equation}
It is flatter on the top when $\mu$ is around 0, and is differentiable at the 
boundary of its support.
Meanwhile, the distance from each neighbor to the target point is calculated 
based on such as Euclidean distance, Great Circle distance, or Mahalanobis 
distance \cite{mahalanobis1936}. Suppose we use Euclidean distance $\|\cdot\|$, 
and $d(x_0)$ is the distance between $x_0$ and its $n_{\alpha}$th nearest neighbor.
Then the weights for each $x_i$ when fitting at $x_0$ are generated as
\begin{equation} 
\omega_i(x_0) = T \left( \frac{\| x_i - x_0 \|}{d(x_0)} \right)
\end{equation}
which assigns 0 weight to $x_i$ outside the circle region with radius of $d(x_0)$.

Once the weights of each $x_i$ are ready for a given $x_0$, the locally fitting
is trivial. For example, let $Y=(y_1, y_2, \cdots, y_n)^{T}$ and 
$\beta = (\beta_0, \beta_1, \beta_2)^{T}$, the local quadratic regression fit at
target point $x_0$ can be illustrated in terms of matrix notation:
\begin{equation}
y = X \beta + \epsilon
\end{equation} 
and the objective function for a given target $x_0$ is: 
\begin{equation}
\label{weightedReg}
\min_{\beta} \;\; (Y - X\beta)^TW(Y - X\beta)
\end{equation} 
where
\begin{equation}
X =  
\begin{pmatrix}
  1 & (x_1-x_0) & (x_1-x_0)^2 \\
  1 & (x_2-x_0) & (x_2-x_0)^2 \\
  \vdots  & \vdots & \vdots  \\
  1 & (x_n-x_0) & (x_n-x_0)^2 
\end{pmatrix}
\end{equation} 
is the design matrix of each local quadratic regression model, and weight diagonal
matrix is
\begin{equation}
W =  
\begin{pmatrix}
  \omega_1(x_0) & 0 & \cdots & 0 \\
  0 & \omega_2(x_0) & \cdots & 0 \\
  \vdots  & \vdots & \ddots & \vdots  \\
  0 & 0 & \cdots & \omega_n(x_0) 
\end{pmatrix}
\end{equation} 
For each target point $x_0$, we need solve the optimization problem in 
equation~\ref{weightedReg}. 


\subsection{General Additive Model}

\section{Geographically Weighted Regression}

\section{Kriging}

\section{Seasonal Trend Decomposition using Loess (STL)}
\label{sec:stl}
The methods and applications in this work deal with seasonal time series
that have trends that are considered to be deterministic, which typically can be
explained by knowledge of the nature of the process. For example, consider the
time series of monthly maximum temperature measured at 8,125 stations all over 
the United States from 1895 to 1997. There are two apparent trends --- one is a
long-term increase. Another trend is the seasonal oscillation within each year.
A major portion of this work deals with methods for modeling time series with
deterministic seasonal and trend components.

Seasonal trend decomposition using loess (STL) is a method for decomposing a time
series into seasonal, trend, and remainder components \cite{Cleveland:1990}.

\subsection{STL with Added Feature}

\section{Divide and Recombine (D\&R) for Large Complex Data}

\subsection{D\&R Statistical Framework}

D\&R \cite{Guha:2012} is a statistical framework for the analysis of large complex
data that enables feasible and practical analysis of large complex data. 

The 
analyst selects a division method to divide the data into subsets, applies an 
analytic method of the analysis to each subset independently with no communication
among subsets, selects a recombination method that is applied to the outputs 
across subsets to form a result of the analytic method for the entire data.

Analytic methods have two distinct categories, visualization methods whose outputs
are visual displays, and number-category methods whose outputs are numeric and 
categorical values. In D\&R, number-category analytic methods are typically applied
to each of the subsets. Visualization methods are typically applied to each subset 
in a sample of subsets because often there are too many of them to look at plots 
of all \cite{Hafen:2013}.
 
\subsection{Computational Environment}

The front end of our computational environment is R \cite{R}, a widely used 
software environment for statistical computing and graphics. On the other side, the
back end is the Hadoop which consists of Hadoop Distributed File 
System (HDFS) \cite{HDFS} for storage and processing engine (MapReduce) 
\cite{mapreduce}. RHIPE \cite{Guha:2010}, the R and Hadoop Integrated Programming 
Environment, bridges the gap between these two ends. 

As analyst, we only need to specify R commands to carry out a D\&R job which
consists of following three steps:
\begin{itemize}
\item Divide the whole dataset into subsets. It can be randomly dividing or
can be done conditional on a given categorical variable. For example, for a spatial
temporal data set, we can either divide the whole data by time or by location. 
\item Apply the analytic method to each subset parallelly.   
\item recombine the outputs of the A computations and write results to the HDFS. 
\end{itemize}  

The first step can be achieved by one MapReduce job using RHIPE R commands which 
creates the subsets from the original raw data set sat on HDFS and 
distributes the subsets across the servers of the cluster onto the Hadoop 
Distributed File System (HDFS) as key-value pairs. Most of situation, the original
raw data can be a raw text file saved on HDFS.

Thereafter, the second and
third steps can be implemented with another MapReduce job also specified by
analyst in RHIPE R commands. In this second MapReduce job, a group of Map
computation procedures are running embarrassingly parallel with a free core 
assigned to each, which means independently with no communication among those Map
procedures. We call those Map procedures the Mapper. Then, a data block or a 
collection of subsets will be passed into those Mappers, and each subset
will be applied with the analytic method independently.

