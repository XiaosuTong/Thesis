\chapter{BACKGROUND}

In this chapter we briefly introduce one type of nonparametric regression method,
namely local polynomial regression method, followed by emphasis on two specific 
generalization of loess as time series decomposition method called Seasonal 
Trend Loess (STL) and Geographically weighted Regression respectively. The main
purpose here is to cover the foundation methodology and principle of the data
analysis proceeded in later chapter, which is crucially depends on these topics.  


\section{Nonparametric Regression}

In the family of regression analysis, the conditional expectation of a response 
variable ($y$) is assumed to be a function of one or several predictors ($x$'s). 
Suppose the data contains $n$ pairs of observations: 
$(x_1, y_1), \cdots, (x_n, y_n)$, the object of regression analysis is to model 
the relationship between $y$ and $x$ in the form of
\begin{equation} 
y_i = f(x_i) + \epsilon_i
\end{equation} 
where $f$ is an unknown function, $\epsilon_i$ is the random error term left in 
the observation which cannot be explained by the model. 
As it is usually proposed in parametric approach, regression analysis assumes a 
known form of function for $f(x_i)$, which is fully described by a finite 
set of parameters. For example, a linear function of predictor variables is 
commonly chosen:
\begin{equation} 
f(x_i) = \beta_0 + \beta_1x_i
\end{equation} 
where $\beta_0$ and $\beta_1$ are the parameters in the model to be estimated. 
The advantage of parametric regression method is that the relationship 
between response variable and predictor variable is described and summarized 
easily by the estimated parameters.

But in real data analysis, $f(x)$ is flexible and unknown, so we may have to 
relate $y$ to $x$ without assuming any pre-defined functional form. And this is 
when nonparametric model is preferred. Concretely, within nonparametric 
regression, the relationship between response variable and predictor does not 
have an explicit form but is constructed based on the information derived from
the data.

\subsection{Local Regression models}

One approach in the nonparametric regression family to modeling an unknown and
complex $f$ is called local regression. Instead of assuming 
$f(x)$ to have a parametric form overall, we assume that the data can be locally, 
within some neighborhood of $x_0$, well-approximated by a parametric form which 
may be polynomials or even a constant. For instance, let a neighborhood of $x_0$ 
be $x_0 \pm h$ for some bandwidth $h > 0$.
Nadaraya \cite{nadaraya1964} and Watson \cite{watson1964} proposed a local 
constant estimator, or kernel regression method, with a pre-defined kernel 
function as weighting function, called Nadaraya-Watson estimator:
\begin{equation} 
\hat f(x_0) = \sum_{i=1}^{n} \omega_{i}y_i
\end{equation} 
where 
\begin{equation} 
\omega_i = \frac{ K(\frac{x_i - x_0}{h})}{\sum_{i=1}^n K(\frac{x_i - x_0}{h})}
\end{equation} 
and $K(\cdot)$ is a non-negative real-valued integrable function, which satisfying 
the following two requirements
\begin{equation} 
\int_{-\infty}^{\infty} K(\mu)d\mu = 1
\end{equation} 
\begin{equation} 
K(-\mu) = K(\mu)
\end{equation}
The same idea can be illustrated when replacement of a quantitative variable by
indicator variables in linear regression. It uses a constant kernel function 
$K(\mu) = 1$ to assign equal weights to all neighbors within $x_0 \pm h$. 

However, no matter which kernel function is chosen, or what the bandwidth $h$ is,
NW estimator has its limitation. It actually fits a weighted constant regression
line in the neighborhood of $x_0$. The prediction variable only effects the weights
of each neighbors of target point $x_0$. In \cite{cleveland1988regression}, 
Cleveland has well indicated that kernel regression always introduces bias in the
estimation of a linear surface, especially on the boundaries of the domain. A more 
sophisticate model than local constant model is to fit a low order polynomial in 
the neighborhood of $x_0$. So the 
predictor variable not only effects the weights of neighbors, it also provides more
information to the local model. In the next subsection, we will present more detail
about local regression model specifically the local polynomial model.

\subsection{Fitting a Local Polynomial Model}
\label{sec:loess}

$loess$, short for locally weighted scatter-plot smoothing, also known as locally
weighted polynomial regression, is a more general and flexible
method than the kernel regression we discussed above. It was originally introduced 
by Cleveland in \cite{Cleveland:1979}, and then well expanded in the references 
of \cite{cleveland1988locally}, \cite{cleveland1991computational}, 
\cite{cleveland1988regression}, and \cite{cleveland1996smoothing} in term of its
methodology, theory, and computation

Suppose we are trying to fit at target point $x_0$ in the predictor space with
one dimension. The loess fit is obtained by locally weighted least square with a
polynomial of degree $\lambda$ using $n_{\alpha}$ nearest neighbors of $x_0$. 
Concretely, a smoothing parameter $\alpha$ is defined between 0 and 1, which is
the ratio of number of observations included in the model as neighbors. Then 
\begin{equation} 
n_{\alpha} = \min(n, \lfloor \alpha n \rfloor)
\end{equation}
A weight is assigned to each neighbors based on their distance to the target $x_0$.
The closer neighbors is to the $x_0$, the higher weight the neighbors will get. The
weight is defined as a function of the distance. Specifically, a tri-cube weight 
function is used.
\begin{equation} 
T(\mu) =
  \begin{cases}
    (1 - \mu^3)^3       & \quad \text{for } 0 \le \mu < 1\\
    0  & \quad \text{for } \mu \ge 1\\
  \end{cases}
\end{equation}
It is flatter on the top when $\mu$ is around 0, and is differentiable at the 
boundary of its support.
Meanwhile, the distance from each neighbor to the target point is calculated 
based on the choice of distance calculation, such as Euclidean distance, Great 
Circle distance, or Mahalanobis distance \cite{mahalanobis1936}. 
Suppose we use Euclidean distance $\|\cdot\|$, and $d(x_0)$ is the distance 
between $x_0$ and its $n_{\alpha}$th nearest neighbor.
Then the weights for each $x_i$ when fitting at $x_0$ are generated as
\begin{equation} 
\omega_i(x_0) = T \left( \frac{\| x_i - x_0 \|}{d(x_0)} \right)
\end{equation}
which assigns 0 weight to $x_i$ outside the circle region with radius of $d(x_0)$.

Once the weights of each $x_i$ are ready for a given $x_0$, the locally fitting
is trivial. Let $Y=(y_1, y_2, \cdots, y_n)^{T}$ and 
$\beta = (\beta_0, \beta_1, \beta_2)^{T}$, the local quadratic regression fit at
target point $x_0$ can be illustrated in terms of matrix notation:
\begin{equation}
Y = X \beta + \epsilon
\end{equation} 
and the minimization of objective function for a given target $x_0$ is the estimate
of parameters. 
\begin{equation}
\label{weightedReg}
\hat \beta = \arg\min \;\; (Y - X\beta)^TW(Y - X\beta)
\end{equation} 
where
\begin{equation}
X =  
\begin{pmatrix}
  1 & (x_1-x_0) & (x_1-x_0)^2 \\
  1 & (x_2-x_0) & (x_2-x_0)^2 \\
  \vdots  & \vdots & \vdots  \\
  1 & (x_n-x_0) & (x_n-x_0)^2 
\end{pmatrix}
\end{equation} 
is the design matrix of each local quadratic regression model, and weight diagonal
matrix is
\begin{equation}
W =  
\begin{pmatrix}
  \omega_1(x_0) & 0 & \cdots & 0 \\
  0 & \omega_2(x_0) & \cdots & 0 \\
  \vdots  & \vdots & \ddots & \vdots  \\
  0 & 0 & \cdots & \omega_n(x_0) 
\end{pmatrix}
\end{equation} 
For each target point $x_0$, we need solve the optimization problem in 
equation~\ref{weightedReg}. It is easy to get the estimate based on weighted
least square:
\begin{equation}
\hat \beta = (X^TWX)^{-1}XWY
\end{equation}
and the estimate of response at $x_0$ is just the intercept of the local quadratic
curve:
\begin{equation}
\hat y_0 = \hat \beta_0
\end{equation}

\subsection{Robust Locally Weighted Regression}

One of shortcoming of local regression methods is its weakness to the effect of 
outliers. In \cite{Cleveland:1979}, a robust local regression procedure was 
proposed. First carry out the original local weighted regression fit at each 
$x_i$, and once we got the $\hat y_i$, residuals defined as following are 
calculated.
\begin{equation}
r_i = y_i - \hat y_i
\end{equation}
Let $B$ be the bi-square function defined as:
\begin{equation} 
\label{bisquare}
B(\mu) =
  \begin{cases}
    (1 - \mu^2)^2      & \quad \text{for } \|\mu\| < 1\\
    0  & \quad \text{for } \|\mu\| \ge 1\\
  \end{cases}
\end{equation}
Let $m$ be the median of the $|r_i|$, and robustness weights is defined as
\begin{equation}
\delta_i = B\left(\frac{r_i}{6m}\right)
\end{equation}
Robustness weights is used to measure the outliers. If there is an outlier in 
the data, the corresponding $r_i$ should be relatively larger than the others.
And we definitely would like to shrink the weight of this outlier in all local
regression fitting which includes it as neighbor. So $\delta_i$ leans to 1
if the residuals normalized by median is small, and is closed to 0 if normalized
residual is large. Then we compute new $\hat y_i$ for each $i$ using the same
local quadratic model but with updated weights
\begin{equation}
\hat \beta = (X^TW_{update}X)^{-1}XW_{update}Y
\end{equation}
where
\begin{equation}
W_{update} =  
\begin{pmatrix}
  \delta_1\omega_1(x_0) & 0 & \cdots & 0 \\
  0 & \delta_2\omega_2(x_0) & \cdots & 0 \\
  \vdots  & \vdots & \ddots & \vdots  \\
  0 & 0 & \cdots & \delta_n\omega_n(x_0) 
\end{pmatrix}
\end{equation}
Finally we repeatedly utilize this procedure with a number of $t$ times to get
the robust locally weighted regression fitted value $\hat y_i$. Usually, four
iteration time is enough to get reliable and converged result.

Because of its flexibility and simplicity, there are various
application of locally weighted regression. In the next two sections, we illustrate
two of main application of loess in different area of data analysis, time series
and spatial analysis respectively.

\subsection{Computation}

The last piece of the loess method is the computation procedure. It will become
more computationally intense to fit at every single observation in the data if 
the size of it is keeping growing. So modification has to be applied to the 
original loess to make the computation more feasible. 

First of all, a kd-tree \cite{friedman1977algorithm} is built to partition the 
predictor space into cells. The number of cells is decided by specifying the 
number of observation in each cell is no more than 
\begin{equation} 
\lfloor\frac{n\alpha}{5}\rfloor
\end{equation} 
Instead of getting local weighted regression estimate at every point,
the directly estimation is only calculated at the vertices. Derivatives of 
$\hat y$ are also estimated at the vertices by taking the slopes of the local 
models. And then cubic Hermite spline is used to interpolate any point within the
corresponding cell.
In \cite{hafen2010local}, Hafen demonstrated that the interpolated fit is very 
close to the exact fit, but make the computation to be linear in $n$.



\section{Seasonal Trend Decomposition using Loess (STL)}
\label{sec:stl}

Seasonal Trend Decomposition using Loess is first introduced by Cleveland in 
\cite{Cleveland:1990}. It is a simple but powerful design for seasonal time
series, which is built on a series of applications of the locally weighted 
regression. It can also be understood as a form of general additive model utilizing
back-fitting algorithm to iteratively update several components of the time series
at each time point.  

Compared with other seasonal adjustment methods of time series such as X-11,
STL method does not just remove the seasonal component from the original 
time series. It actually decomposes time series into seasonal, trend, and remainder
components, so the property and variability of each component including seasonal 
is able to be analyzed. 

\subsection{STL Procedure}

As a filtering procedure, STL is trying to decompose a time series 
$y_i$, $i=1, \cdots, n$, to be
\begin{equation} 
\label{stlModel}
y_i = t_i + s_i + r_i
\end{equation}
where $t_i$, $s_i$, and $r_i$ are the trend, seasonal, and remainder components
for $i$th time point respectively. With an pre-defined initial value of $s_i$ and 
$t_i$ (usually set to be 0), STL starts to smooth the seasonal and trend components iteratively, and the remainder is whatever left in $y_i$ besides the other two. 
The seasonal component has to be determined based on a prior knowledge of the time
series itself, which is the seasonal periodicity. For example, the dataset we will
demonstrate in the second chapter is the about monthly maximum temperature in the
United State. There are 12 observations in each annual period, so the $n_{(p)}$ 
is equal to 12. If the time series is hourly temperature observation, then the
$n_{(p)}$ is 24, and so forth.

STL is comprised of two nested iterative procedures, an outer loop with an inter 
loop inside. Collectively, all smoothing operation in STL are based on loess method
heavily. Smoothing parameters are required to be specified for each smoothing 
operation, and the smoothing parameter $\alpha$ is replaced by window size
$w=\lfloor \alpha n\rfloor$ in those operation, which is just specifying the 
number of observation used in local model instead of the ratio.  

First in the smoothing procedure of seasonal component, the original time series 
is divided into $n_{(p)}$ $cycle$-$subseries$, each of which is defined to be 
the subseries at each time point of the seasonal cycle \cite{hafen2010local}, 
\cite{Cleveland:1990}. For instance, the date of monthly maximum temperature 
observation, all the observations of January will be the first subseries, and
all values of February is the second subseries and so forth, so totally there are 
12 subseries for monthly data. Each subseries is smoothed separately and 
independently and then recombined together. Next, smoothing procedure of 
trend component is applied to the $deseasonalized$ series, which exclusive the 
seasonal component fit from the $y_i$. The outer loop starts with the inner loop
in which two components are estimated back and forth using back-fitting algorithm 
\cite{breiman1985estimating}. After the inner loop, a robustness weight is 
calculated for each time $i$ before the end of current iteration of outer loop,
and it will be passed into the next iteration.

\subsubsection{Inner loop}

The inner loop is basically the estimation procedure between seasonal and trend
components. Concretely, the trend component is initialized as 0, then the seasonal
component is obtained by applying smoothing separately to each $cycle$-$subseries$ 
of the $detrending$ series using local weighted regression with window size $w_s$ 
and degree $d_s$; trend component is calculated by another local weighted smoothing
to the $deseasonalized$ series with window size $w_t$ and degree $d_t$. Moreover, 
seasonal and trend smoothing procedure can be also treated as two filtering 
procedure. Unfortunately two filtering procedure compete with each other the 
variation in the series at low frequencies. So another high-pass filtering procedure
is applied before the trend smoothing.

There are mainly 5 steps in the inner loop, and these steps will be iterated for
$n_{(I)}$ times. In each iteration time $k$, we utilize the same smoothing parameter,
which are $w_s$ and $d_s$, to get the current estimate of seasonal component 
$s_i^{(k)}$, and use smoothing parameter $w_t$ and $d_t$ to get the current 
estimate of trend component $t_i^{(k)}$. Details are illustrated as following:

\begin{enumerate}
  \item $Detrending$:
  We start with subtracting the previous estimation of trend component from the
  $y_i$: $y_i - t_i^{(k-1)}$. If $k=1$, then we initialize $t_i^{(0)}=0$. Notice,
  if $y_i$ is missing, the detrended series is also missed at the same time point.
  \item $Cycle$-$subseries$ $Smoothing$:
  A loess smoothing procedure is applied to each $subseries$ of the detrended 
  series $y_i - t_i^{(k)}$ with smoothing window size $w_s$ and smoothing degree
  $d_s$. Smoothed value is calculated at every time point including the time with
  missing value, and also at the time point one prior to the first observation
  and one after the last observation of the original subseries. Consequently,
  all $n_{(p)}$ smoothed subseries are pulled together to be the 
  temporal seasonal series $C_i^{(k)}$ with length of $n + 2n_{(p)}$, because we
  extrapolated each of $n_{(p)}$ subseries with one extra fitting value on each 
  side. $i$ is from $-n_{(p)} + 1$ to $n + n_{(p)}$.  
  \item $Low$-$pass$ $Filtering$:
  It seems to be reasonable to just treat $C_i^{(k)}$ as the seasonal component.
  However, if we view the smoothing procedure which generates $c_i^{(k)}$, it
  actually also captured low-frequency variation which truly should belong into 
  the trend component. So a low-pass filter is applied to $c_i^{(k)}$ to capture 
  any low-frequency variation, or any long-term trend in $c_i^{(k)}$, which we 
  denote as $l_i^{(k)}$. Specifically, this low-pass filtering is composed of 
  three moving average procedure with length of $n_{(p)}$, another $n_{(p)}$, and 
  3 respectively. Since moving average drop the ending points, the length of the 
  series is back to $n$ after them. Then a loess smoothing procedure with parameter 
  $w_l$ and $d_l$ is applied to generate the $l_i^{(k)}$ series. Finally the 
  seasonal component is obtained as $s_i^{(k)} = c_i^{(k)} - l_i^{(k)}$.  
  \item $Deseasonalizing$:
  Once the seasonal component is estimated, it is subtracted out from $y_i$. If
  $y_i$ is missing, then $y_i - s_i^{(k)}$ is also kept as missing at corresponding
  time point.
  \item $Trend$ $Smoothing$:
  The last step for inner loop is applying a loess smoothing with parameter $w_t$
  and $d_t$ to the deseasonalized series $y_i - s_i^{(k)}$. Smoothed value is 
  calculated at all time points including those with missing values 
\end{enumerate}

The inner loop is run $n_{(I)}$ times, and remainder is calculated based on the
estimates of seasonal component $\hat s_i$ and trend component $\hat t_i$:
\begin{equation} 
\hat r_i = y_i - \hat t_i - \hat s_i
\end{equation}

\subsubsection{Outer loop}

It is very likely to have an outlier in the given time series. For example, like
the monthly maximum temperature data in chapter two, extremely hot or cold weather 
always exist. So similar with robust locally weighted regression mentioned 
previously, robust weights are calculated to lessen the effect of outliers.

Let $m$ be the median of the $|r_i|$, and robustness weights is defined as
\begin{equation}
\delta_i = B\left(\frac{r_i}{6m}\right)
\end{equation}
$B$ is still the bi-square function defined in \ref{bisquare}. So the weights
for each observation used in the inner loop is now multiplied by the robustness
factor $\delta_i$. The inner loop and robustness calculation together form the
outer loop, and it iterates for $n_{(O)}$ times.

\subsection{Smoothing Parameter of STL}

Collectively, there are 8 smoothing parameters needed to be specified in STL
procedure besides $n_{(p)}$, which are:$w_s$, $d_s$ for seasonal smoothing, 
$w_l$, $d_l$ for the low-pass filtering procedure, $w_t$, $d_t$ for trend 
smoothing, and iteration time $n_{(I)}$ and $n_{(O)}$. Fortunately, most of 
smoothing parameters have very nice default value except $w_s$, $d_s$, and $w_t$.
In the current implantation of STL in base R called $stl$ function, $d_s$ and 
$d_t$ are restricted to be either local constant or local linear fit, is not very
efficient for complex seasonal or trend component. It is reasonable to restrict
$d_l$ for low-pass filter to be local linear since we absolutely do not want any
variation with high frequency is filtered out from seasonal component. As 
illustrated in \cite{Cleveland:1990}, by setting $w_l$ to be the least odd integer 
greater than or equal to $n_{(p)}$ can help to preclude the competing for same
variation between trend and seasonal component. With respect to the iteration time,
robust fit is needed if extreme value behavior exist based on the domain knowledge
of the data. With $n_{(I)}=1$, 5 will be a fair value for $n_{(O)}$, otherwise,
$n_{(I)}=2$ is sufficient for non-robust fit in general.

\subsubsection{Diagnostic Plots}

Visualization diagnostic plots are very helpful for us to decide the value of 
$w_s$, $d_s$, and $w_t$. For estimate of seasonal component, $s_i + r_i$ and 
centralized $s_i$ are plotted against to time $i$ conditional on each subseries 
which is called as $Seasonal$-$Diagnostic$ $plot$. It helps us to balance the
bias-variance tradoff within seasonal smoothing procedure. Examples can be found
in \cite{Cleveland:1990}.
For estimate of trend component $\hat t_i$, $t_i + r_i$ and $t_i$ itself are 
plotted against to time $i$. By assessing the visual graph, we can tell if too
much variation (high variance) or very few variation (high bias) is included in
the trend component.

\subsection{STL with Added Feature in $stlplus$}

In \cite{hafen2010local}, Hafen demonstrated a new implementation of STL procedure
in R called $stlplus$ package. Compared with original $stl$ function in base R,
it has following benefits:
\begin{enumerate}
\item Enable local quadratic fit for seasonal and trend component. Based on the
behavior of the time series, it is very likely we need local quadratic fitting
to the smoothing procedure.
\item Be able to handle missing value. Even though the STL procedure illustrated
in \cite{Cleveland:1990} is able to handle missing values, the old implementation
$stl$ function in base R cannot. $stlplus$ function now is capable to fit at
any missing value time point.
\item Give a theoretical lower bound for smoothing window for local quadratic fit.
In the original STL article \cite{Cleveland:1990}, a lower bound for $w_t$, and
$w_l$ is provided only for local linear fit. In the $stlplus$ implementation, 
however, \cite{hafen2010local} provides theoretical lower bound for $w_t$, $w_l$ 
when utilizing local quadratic fit for seasonal and trend smoothing.
\item Blending endpoints to lower degree polynomial. The difficulty of smoothing 
at the endpoints is inherent in all local regression procedures because the first 
and last observations of the series are not smoothed with the same symmetric 
weights that are used in the center of the series. And endpoints problem cannot be
neglected for forecasting for future observation in time series.
\end{enumerate}



\section{Geographically Weighted Regression}

\section{Divide and Recombine (D\&R) for Large Complex Data}

\subsection{D\&R Statistical Framework}

D\&R \cite{Guha:2012} is a statistical framework for the analysis of large complex
data that enables feasible and practical analysis of large complex data. 

The 
analyst selects a division method to divide the data into subsets, applies an 
analytic method of the analysis to each subset independently with no communication
among subsets, selects a recombination method that is applied to the outputs 
across subsets to form a result of the analytic method for the entire data.

Analytic methods have two distinct categories, visualization methods whose outputs
are visual displays, and number-category methods whose outputs are numeric and 
categorical values. In D\&R, number-category analytic methods are typically applied
to each of the subsets. Visualization methods are typically applied to each subset 
in a sample of subsets because often there are too many of them to look at plots 
of all \cite{Hafen:2013}.
 
\subsection{Computational Environment}

The front end of our computational environment is R \cite{R}, a widely used 
software environment for statistical computing and graphics. On the other side, the
back end is the Hadoop which consists of Hadoop Distributed File 
System (HDFS) \cite{HDFS} for storage and processing engine (MapReduce) 
\cite{mapreduce}. RHIPE \cite{Guha:2010}, the R and Hadoop Integrated Programming 
Environment, bridges the gap between these two ends. 

As analyst, we only need to specify R commands to carry out a D\&R job which
consists of following three steps:
\begin{itemize}
\item Divide the whole dataset into subsets. It can be randomly dividing or
can be done conditional on a given categorical variable. For example, for a spatial
temporal data set, we can either divide the whole data by time or by location. 
\item Apply the analytic method to each subset parallelly.   
\item recombine the outputs of the A computations and write results to the HDFS. 
\end{itemize}  

The first step can be achieved by one MapReduce job using RHIPE R commands which 
creates the subsets from the original raw data set sat on HDFS and 
distributes the subsets across the servers of the cluster onto the Hadoop 
Distributed File System (HDFS) as key-value pairs. Most of situation, the original
raw data can be a raw text file saved on HDFS.

Thereafter, the second and
third steps can be implemented with another MapReduce job also specified by
analyst in RHIPE R commands. In this second MapReduce job, a group of Map
computation procedures are running embarrassingly parallel with a free core 
assigned to each, which means independently with no communication among those Map
procedures. We call those Map procedures the Mapper. Then, a data block or a 
collection of subsets will be passed into those Mappers, and each subset
will be applied with the analytic method independently.

