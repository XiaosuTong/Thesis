\chapter{BACKGROUND}

This chapter introduces local regression and some time series modeling methods,
with emphasis on a time series decomposition method: seasonal trend decomposition
using loess (STL), and a spatially local regression method. The goal is to be
brief but still cover the methods thoroughly, as much of the new work developed
later depends heavily on these topics.

\section{Nonparametric Regression}

Regression analysis traces the mean value of a response variable ($y$) as a 
function of one or several predictors ($x$'s). Suppose that there is $n$ pairs 
of observations: $(x_1, y_1), \cdots, (x_n, y_n)$, the object of regression 
analysis is to model the relationship between $y$ and $x$ in the form of
\begin{equation} 
y_i = f(x_i) + \epsilon_i
\end{equation} 
where $f$ is an unknown function, $\epsilon_i$ is the random error term in the
observation which cannot be explained by the model. 
As it is usually practiced in parametric approach, regression analysis assumes a 
known form of function for $f(x_i)$, which is fully described by a finite 
set of parameters, to be estimated. For example, linear function of predictor 
variables:
\begin{equation} 
f(x_i) = \beta_0 + \beta_1x_i
\end{equation} 
where $\beta$s are the parameters in the model to be estimated. The relationship 
between response variable and predictor variable is described and summarized 
easily by the estimated parameters.

But sometimes $f(x)$ is flexible and unknown, we have to relate $y$ to $x$s 
without assuming any functional form. And this is when nonparametric model comes 
in. Concretely, nonparametric regression is a form 
of regression analysis in which the predictor does not take a predetermined form 
but is constructed according to information derived from the data.

\subsection{Local Regression models}

One approach from the nonparametric regression family to modeling an unknown and
complex $f$ is local regression \cite{Cleveland:1979}. Instead of assuming 
$f(x)$ to have a parametric form overall, we assume that the data can locally, 
within some neighborhood of $x$, be well-approximated by a parametric form which 
can be polynomial or even constant. For instance, let a neighborhood of $x$ be
$x \pm h$ for some bandwidth $h > 0$.
Nadaraya \cite{nadaraya1964} and Watson \cite{watson1964} proposed a local 
constant estimator with a pre-defined kernel function as weighting function, 
called Nadaraya-Watson estimator:
\begin{equation} 
\hat f(x) = \frac{\sum_{i=1}^n K(\frac{x_i - x}{h})y_i}{\sum_{i=1}^n K(\frac{x_i - x}{h})}
\end{equation} 
where $K$ is non-negative real-valued integrable function, which satisfying the 
following two requirements
\begin{equation} 
\int_{-\infty}^{\infty} K(\mu)d\mu = 1
\end{equation} 
\begin{equation} 
K(-\mu) = K(\mu)
\end{equation}
The same idea can be illustrated when replacement of a quantitative variable by
indicator variables. It uses a constant kernel function $K(\mu) = 1$ to assign
equal weights to all neighbors within $x\pm h$. 

However, no matter which kernel function is chosen, or what the bandwidth $h$ is,
NW estimator has its limitation. It actually fits a weighted constant regression
line in the neighborhood of $x$. The prediction variable only effects the weights
of each neighbors of target point $x$. A more general model than local constant
model is to fit a low order polynomial in the neighborhood of $x$. So the predictor
variable not only effects the weights of neighbors, but also the local model itself.


\subsection{Fitting a Local Regression Model}
\label{sec:loess}

\subsection{Computation}

\subsection{General Additive Model}

\section{Geographically Weighted Regression}

\section{Kriging}

\section{Seasonal Trend Decomposition using Loess (STL)}
\label{sec:stl}
The methods and applications in this work deal with seasonal time series
that have trends that are considered to be deterministic, which typically can be
explained by knowledge of the nature of the process. For example, consider the
time series of monthly maximum temperature measured at 8,125 stations all over 
the United States from 1895 to 1997. There are two apparent trends --- one is a
long-term increase. Another trend is the seasonal oscillation within each year.
A major portion of this work deals with methods for modeling time series with
deterministic seasonal and trend components.

Seasonal trend decomposition using loess (STL) is a method for decomposing a time
series into seasonal, trend, and remainder components \cite{Cleveland:1990}.

\subsection{STL with Added Feature}

\section{Divide and Recombine (D\&R) for Large Complex Data}

\subsection{D\&R Statistical Framework}

D\&R \cite{Guha:2012} is a statistical framework for the analysis of large complex
data that enables feasible and practical analysis of large complex data. 

The 
analyst selects a division method to divide the data into subsets, applies an 
analytic method of the analysis to each subset independently with no communication
among subsets, selects a recombination method that is applied to the outputs 
across subsets to form a result of the analytic method for the entire data.

Analytic methods have two distinct categories, visualization methods whose outputs
are visual displays, and number-category methods whose outputs are numeric and 
categorical values. In D\&R, number-category analytic methods are typically applied
to each of the subsets. Visualization methods are typically applied to each subset 
in a sample of subsets because often there are too many of them to look at plots 
of all \cite{Hafen:2013}.
 
\subsection{Computational Environment}

The front end of our computational environment is R \cite{R}, a widely used 
software environment for statistical computing and graphics. On the other side, the
back end is the Hadoop which consists of Hadoop Distributed File 
System (HDFS) \cite{HDFS} for storage and processing engine (MapReduce) 
\cite{mapreduce}. RHIPE \cite{Guha:2010}, the R and Hadoop Integrated Programming 
Environment, bridges the gap between these two ends. 

As analyst, we only need to specify R commands to carry out a D\&R job which
consists of following three steps:
\begin{itemize}
\item Divide the whole dataset into subsets. It can be randomly dividing or
can be done conditional on a given categorical variable. For example, for a spatial
temporal data set, we can either divide the whole data by time or by location. 
\item Apply the analytic method to each subset parallelly.   
\item recombine the outputs of the A computations and write results to the HDFS. 
\end{itemize}  

The first step can be achieved by one MapReduce job using RHIPE R commands which 
creates the subsets from the original raw data set sat on HDFS and 
distributes the subsets across the servers of the cluster onto the Hadoop 
Distributed File System (HDFS) as key-value pairs. Most of situation, the original
raw data can be a raw text file saved on HDFS.

Thereafter, the second and
third steps can be implemented with another MapReduce job also specified by
analyst in RHIPE R commands. In this second MapReduce job, a group of Map
computation procedures are running embarrassingly parallel with a free core 
assigned to each, which means independently with no communication among those Map
procedures. We call those Map procedures the Mapper. Then, a data block or a 
collection of subsets will be passed into those Mappers, and each subset
will be applied with the analytic method independently.

