\section{Database of Division by Month}

In the previous section, we illustrated the results of exploratory analysis.
Recall in the Figure \href{../plots/obs_month.pdf}{\ref*{obs.month}}, we notice
the number of active stations gradually stabilized after Jan 1950. Since long
period of missing observation is not the main concern of this thesis, we decide
only consider stations with observations after Jan 1950.

The first database we create for data after 1950 is called division by month. In
the following paragraph we describe the generation of such database with details.

\begin{description}
  \item[Input] 10,042,500 key-value pairs from the initial database, key is the
  vector with combination of 8,125 unique \texttt{station.id} and one of 1,236 
  month index, and a one-row R data.frame as the value. The value includes 
  (1) \texttt{longitude}, (2) \texttt{latitude}, (3) \texttt{elevation}, (4) 
  \texttt{year}, (5) \texttt{month}, month of year (January to December), (6) 
  \texttt{tmax}, the monthly maximum temperature.
  \item[Output] division by month in form of 576 key-value pairs, with vector of
  \texttt{year} and \texttt{month} as the key, a R data.frame containing 
  observations from 7,738 stations in the corresponding month as the value.
  \item[Map]For every input key-value pair, we include a new column named 
  \texttt{station.id} to the data.frame, and remove the \texttt{year} and 
  \texttt{month} from the value to be the key of intermediate key-value pairs. 
  \item[Reduce] 7,738 of one-row data.frame corresponding to one month are 
  aggregated to be one data.frame. Specifically all one-row data.frame who share
  the same \texttt{year} and \texttt{month} are shuffled and transferred to one 
  reducer, and then R function \texttt{rbind} is called in that reducer to combine 
  those one-row data.frame.
\end{description}


\subsection{Distance Calculation of Neighbors}

As we mentioned in section~\ref{sec:loess}, Loess method calculates the weights 
$\omega_i(x)$
for weighted least square based on the distance of neighbors $x_i$ to the target 
point $x$. The distance calculation of $d(x_i, x)$ is based on Euclidean distance 
(in two dimension for example):
$$
d(x_i, x) = \| x - x_i \|_2
$$

Treating geodetic coordinates as
planar can induce deceptive anisotropy in the models because
of the difference in differentials in longitude and latitude (a
unit increment in degree longitude is not the same length as
a unit increment in degree latitude except at the equator) \cite{banerjee2005geodetic}.

In the spatial loess fitting, however, the Euclidean distance is apparently not 
appropriate. A more reasonable and realistic distance definition is the Great-circle 
distance, which is the shortest distance between two points on the surface of a 
sphere, measured along the surface of the sphere (as opposed to a straight line 
through the sphere's interior). The distance between two points in Euclidean 
space is the length of a straight line between them, but on the sphere there are 
no straight lines. In non-Euclidean geometry, straight lines are replaced with 
geodesics. Geodesics on the sphere are the great circles (circles on the sphere 
whose centers coincide with the center of the sphere).

Through any two points on a sphere which are not directly opposite each other, 
there is a unique great circle. The two points separate the great circle into 
two arcs. The length of the shorter arc is the great-circle distance between the 
points\cite{greatcircle}.

Explicitly, let $\phi_a$, $\lambda_a$ and $\phi_b$, $\lambda_b$ be the geographical 
latitude and longitude of two points $a$ and $b$, and $\Delta \phi$, 
$\Delta \lambda$ their absolute differences; then $\Delta \sigma$ the central angle 
between them, is given by:
$$
\Delta \sigma_{a,b} = 2 \arcsin \sqrt{ \sin^2 ( \frac{\Delta \phi}{2} ) + \cos 
\phi_a \cdot \cos \phi_b \cdot \sin^2 ( \frac{\Delta \lambda}{2} )}
$$ 
Then the distance $d(a,b)$, i.e. the arc length, for a sphere of radius $r$ is
$$
d(a,b) = r \Delta \lambda_{a,b}
$$

\subsection{Spatial Smoothing without Elevation}
\label{sec:impute.w/o.elev}

As a practical matter, for many of the infill cases, records from the previous and
following months are not available and therefore a model that includes a temporal 
component will not improve the estimates. Stein (1999) suggests that the
best linear unbiased prediction at a point of interest, when using a stationary 
model, depends on local behavior of the random field far more than on behavior at 
locations far from the point of interest and is one justification for simplifying 
the infilling over an irregular lattice of time and space.

As the distance function is well defined, based on which we start the imputation 
of missing value by spatial loess against to longitude and latitude. Concretely,
the missing value in each month is imputed by its neighbors using locally weighted
regression against to longitude and latitude of each location. Moreover, imputation
in each month are carried out independently in parallel using database of division
by month as input, and generate a new by month division with missing value imputed.

Next we illustrate the MapReduce job which proceed the imputation in parallel.
\begin{description}
  \item[Input] 576 key-value pairs of by month division. Key is a vector of
  \texttt{year} and \texttt{month}; value is a data.frame object with 7,738 
  number of rows, each of which is observation for one station.
  \item[Output] 576 key-value pairs. Key is same as input, and value is still
  a data.frame but with missing value been imputed. 
  \item[Map]For every input key-value pair, we carry out the spatial loess 
  imputation at every location with missing value in that month. 
  \item[Reduce] Reduce is not needed in this MapReduce job.
\end{description}

In the Map step, the spatial loess imputation is proceeded by a new R package 
named \texttt{Spaloess}. This package is wrote based on the \texttt{loess} function
in the base R. The original LOESS implementation in R, available by \texttt{loess},
is just a wrapper around the C and FORTRAN code. This implementation only allows
for Euclidean distance when calculated the weights for neighbors. Also it does
not predict missing values in the data. The main function in the \texttt{Spaloess} 
package is \texttt{spaloess}, which has following added new feature compared with 
\texttt{loess}:
\begin{itemize}
  \item allow two different type of distance calculation: Euclidean distance or
  Great-circle distance. Of course, if Great-circle distance is chosen, the 
  predictor variables must be longitude and latitude, or elevation.
  \item can handle and predict the NA in the input data, controlled by a logic
  argument.
\end{itemize}

The parameters needed for the spatial loess are span, which control the percent
of neighbors used in the regression, and degree controls the polynomial degree of
the regression. 
After the previous MapReduce job, there is a data.frame as value of key-value 
pairs including the original maximum temperature observation as well as the 
imputed value of all stations for each month. In the next section, we demonstrate
the spatial imputation with different parameters, and assess the imputation by
visualization method.

Then we carry out another MapReduce job to generate visual graph in parallel. 
However the visualization methods are typically applied to each subset in a 
sample of subsets, the sampled 128 stations, because there are too many of them 
to look at plots of all \cite{Hafen:2013}.
Overall, there are two types of visual graph to assess the spatial imputation. The
first type is generating visual display for each year. Visualization of twelve
months in a given year is plotted on one page with twelve panels. The layout of
panels maybe different depends on the visualization itself. For this type of 

Next we illustrate the MapReduce job which proceed the first type of visual graph
in parallel.

\begin{description}
  \item[Input] 576 key-value pairs of by month division. Key is a vector of
  \texttt{year} and \texttt{month}; value is a data.frame object with 7,738 
  number of rows, which includes original observation of maximum temperature
  and the imputed value. Also the residual of imputation for each station is 
  calculated.
  \item[Output] 48 key-value pairs. Key is \texttt{year}, and value is a serialized
  trellis object for a given year created by \texttt{serialize} function.
  \item[Map]For every input key-value pair, the key is changed from a vector of 
  \texttt{year} and \texttt{month} to \texttt{year} only. The value is kept the
  same as input.
  \item[Reduce] Intermediate key-value pairs who share the same \texttt{year} are
  shuffled and send to one reduce, and then twelve data.frame are row bind 
  together accumulatively. A predefined plotting engine function defined based on 
  functions in \texttt{lattice} package is passed into each reducer and apply to 
  the accumulated data.frame, which generates a \texttt{trellis} object for each 
  year. In the end, \texttt{serialize} function is applied to the trellis object 
  and saved on HDFS.
\end{description}

As we will see later in Figure 
\href{../plots/a1950/spaimpute/nonelev/span0.05/a1950.spaResidcenter.bytime.pdf}
{\ref*{nonelev.0.05.Residcenter.bytime}} which belongs to the first type of 
visualization, \texttt{trellis} object of each year is generated on one page in 
the Reduce step by calling a specific visualization function in \texttt{lattice} 
package, for example, \texttt{xyplot} or \texttt{qqmath}.

The Second type of visualization is to assess the imputation for each of 576 month
on one page. Specifically, a serialized \texttt{trellis} object is created for 
each month. We illustrate this in the next MapReduce job.

\begin{description}
  \item[Input] 576 key-value pairs of by month division. Key is a vector of
  \texttt{year} and \texttt{month}; value is a data.frame object with 7,738 
  number of rows, which includes original observation of maximum temperature
  and the imputed value. Also the residual of imputation for each station is 
  calculated.
  \item[Output] 576 key-value pairs. Key is same as input, and value is a serialized
  trellis object created by \texttt{serialize} function.
  \item[Map]For every input key-value pair, a predefined plotting engine function
  defined based on functions in \texttt{lattice} package is passed into and apply 
  to the value, which generate a \texttt{trellis} object for each key-value pair. 
  Then \texttt{serialize} function is applied to the trellis object in order to 
  save it as the value of intermediate key-value pairs.
  \item[Reduce] Reduce is not needed in this MapReduce job. Intermediate key-value
  pairs are directly save to HDFS.
\end{description}

\subsubsection{span = 0.05 and degree = 2}

In the first run, we specify the span of spatial loess to be 0.05, which means
for each missing value, it will be imputed by using 5 percent of valid observation
in its neighborhood. And the polynomial degree of spatial loess fit is set to be
quadratic.

\begin{framed}
\begin{center}
  \href{../plots/a1950/spaimpute/nonelev/span0.05/a1950.spaResidcenter.bytime.pdf}
  {Link to figure}
  \captionof{figure}
  {The normal quantiles between 0.015 and 0.985 of residual of spatial imputation}
  \label{nonelev.0.05.Residcenter.bytime}
\end{center}
\end{framed}

For each month, there are around 5,000 imputation residuals. 971 quantiles of the
imputation residuals, from 0.015 to 0.985, are plotted against to their corresponding 
unit Normal quantiles in Figure 
\href{../plots/a1950/spaimpute/nonelev/span0.05/a1950.spaResidcenter.bytime.pdf}
{\ref*{nonelev.0.05.Residcenter.bytime}}. Notice that the range of the residual
for each month is from -5 to 5 collectively, and it has longer tail compared with
unit Normal distribution. Besides the Normal quantile plot, another type of 
diagnostics plot to assess the imputation of missing value is the scatter plot of
imputation residual against to one of the predictor variables conditional on the
other one, specifically longitude and latitude, which will be shown in the following. 
As in Figure
\href{../plots/a1950/spaimpute/nonelev/span0.05/a1950.spaResid.vs.lat.lon.pdf}
{\ref*{nonelev.0.05.Residvslat.lon}}, when longitude is being conditional on, it
is equally cut into categorical variable with 20 intervals. Then imputation 
residual is plotted against to latitude in each interval of longitude in each of
20 panels. Also a loess smoothing curve, with span is equal to 0.25 and degree is
equal to 1, is added to help us assess the overall trend of the residuals. The 
loess smoothing curve is collectively around the horizontal reference line at zero
over all month. However, it is noticeable that most of large negative residuals are
regularly show up in the first five panels over all months. The first five panels
are for the longitude from -124.7 to -110.9 degree. Recall that this region is
the Rocky Mountains where the elevation is various dramatically.
Moreover, there is an outlier with relatively large negative residual consistently 
in the panel with largest longitude. The corresponding location of this outlier
is station \texttt{275639} with elevation 1909 meters, longitude -71.30 degree
and altitude 44.27 degree. An extremely important fact with respect to this
station is that it is the only station with elevation above 1200 meters among
643 stations in its neighborhood in the northeastern corner of the United States. 

\begin{framed}
\begin{center}
  \href{../plots/a1950/spaimpute/nonelev/span0.05/a1950.spaResid.vs.lat.lon.pdf}
  {Link to figure}
  \captionof{figure}
  {The residual of spatial imputation against to latitude conditional on longitude}
  \label{nonelev.0.05.Residvslat.lon}
\end{center}
\end{framed}

Same information can be found in Figure 
\href{../plots/a1950/spaimpute/nonelev/span0.05/a1950.spaResid.vs.lon.lat.pdf}
{\ref*{nonelev.0.05.Residvslon.lat}} as well. Variation of the residual is wider
in the west side of United State compared with east side cross all the 20 different
levels of latitude degree. Again the station \texttt{275639} with elevation 1909 
meters can be found in the 17th panel of latitude. Another group of outliers can 
be found in the 6th panel of latitude, which is in the region of 35.5 to 36.5 degree, 
around longitude -82 degree. It is the region of Appalachian Mountains.

\begin{framed}
\begin{center}
  \href{../plots/a1950/spaimpute/nonelev/span0.05/a1950.spaResid.vs.lon.lat.pdf}
  {Link to figure}
  \captionof{figure}
  {The residual of spatial imputation against to longitude conditional on latitude}
  \label{nonelev.0.05.Residvslon.lat}
\end{center}
\end{framed}

Intuitively, we want to illuminate the amount of bias in the imputation. We have 
to sacrifice the variation in the bias-variance trade-off of the local regression
method. Meanwhile, elevation is another potential critical predictor variable to
impute the maximum temperature based on the diagnostics plots above.
Consequently, we need to consider a new imputation model which includes elevation as
one of the predictor variables, also shrink the span value to illuminate the 
bias in the imputation.

\subsection{Spatial Smoothing with Elevation}

As we mentioned previously, elevation should be considered to be included in the 
local regression model. However, more thoughts should be given to the distance 
calculation when elevation is added into the model. If we simply add elevation
in the local regression model directly, the calculation of weights of neighbors
needs modification based on the three-dimension predictor space. However, the 
definition of distance in this three-dimension space is not trivial, since the
temperature is not isotropic in elevation direction and in the surface. As a 
result, Euclidean distance in three-dimension space is not appropriate for this
situation. Meanwhile, it is more intuitively to assume that the neighborhood or
distance to neighbors are defined only based on longitude and latitude, as the
weights introduced in Geographically Weighted Regression \cite{fotheringhamGWR}.
In the next subsection, we will describe a way to include elevation variable as
a global parameter into the local regression model.

\subsubsection{Conditionally Parametric Model}

The conditionally parametric fit, introduced in \cite{cleveland1992local}, provides
a generalization of the semi-parametric model, without the complexity of a full
local fit. By definition, We are trying to fit a nonparametric surface of maximum 
temperature based on longitude, latitude, and elevation, which are three predictor
variables. A nonparametric surface is conditionally parametric if we can divide
the predictor variables up into two disjoint subsets \texttt{A} and \texttt{B} 
with the following property: given the values of the variables in \texttt{A},
the surface is a member of a parametric class as a function of the the variables
in \texttt{B}. We say that the surface is conditionally parametric in \texttt{A}.
It makes sense to specify a regression surface to be conditionally parametric
in one or more variables if exploration of the data or a priori information
suggests that the underlying pattern of the data is globally a very smooth
function of the variables. Making such a specification when it is valid can
result in a more parsimonious fit.
An exceedingly simple modification of loess fitting yields a conditionally
parametric surface. We simply ignore the conditionally parametric factors
in computing the Euclidean distances that are used in the definition of the
neighborhood weights, $W_i$(x).

The method for making a loess fit conditionally parametric in a proper subset of
predictors is simple. The subset is ignored in computing the Euclidean distances 
that are used in the definition of the neighborhood weights, $\omega_i(x)$. Let 
us use an example to explain why this produces a conditionally parametric surface. 
Suppose that there are two predictors, $\mu$ and $\nu$, and $\lambda=2$. Suppose 
we specify $\mu$ to be a conditionally parametric predictor. Since the weight 
function ignores the variable $\mu$ the $i$th weight, $\omega_i(\mu, \nu)$ for 
the fit at $(\mu, \nu)$, is the same as the $i$th weight, $\omega_i(\mu+t, \nu)$, 
for the fit at $(\mu+t, \nu)$. Thus the quadratic polynomial that is fitted locally 
for the fit at $(\mu, \nu)$ is the same as the quadratic polynomial that is fitted 
locally for the fit at $(\mu+t, \nu)$, whatever the value of $t$. So for the fixed 
value of $\nu$, the surface is exactly this quadratic as a function of the first 
predictor.

A growing list of practical applications has shown that conditionally parametric
fits add structure in a way that is quite useful in practice 
\cite{cleveland1991computational} and \cite{hastie1990generalized}. This is to be
expected because there is a good argument for why we would expect conditionally
parametric surfaces to frequently provide a good fit. If a surface has, instead
of no effect, a very limited effect due to these factors compared to other factors,
it is reasonable to expect that the effect would be nearly linear or quadratic
in these factors given the others. One way to argue for this is to think of Taylor
series approximations. Consider the maximum temperature surface as a function 
over all imaginable values of C and E. Suppose it is very complex function of both
C and E with many peaks and valleys. Taylor series arguments make it clear that
C can be varied by a sufficiently small amount in an experiment so that the 
surface is well approximated by a function that is constant in C given E. If the
domain of variation of C is enlarged, the next approximation is a linear function
of C given E, and then quadratic. This argument carries with it a corollary; we 
would expect that, overall, the influence of the variables in which the surface
is conditionally parametric would be less than the influence of the other 
variables. 

The conditionally parametric model is similar to the partially linear model,
in that the fit is smooth in some variables and parametric in others. But
the conditionally parametric fit allows all coefficients of the parametric
variables to depend on the smoothing variables. A conditionally quadratic
fit has the form

\subsubsection{span=0.015, degree=2}

Based on the result we found in the previous subsection, we modify the span of 
spatial imputation to be 0.015, which means about 75 locations in the neighborhood
of the target location will be used for local weighted regression fit if there 
are roughly 5,000 locations in total. The degree for longitude and latitude 
variables are kept as quadratic, and the degree for conditional parameter 
elevation is also specified as quadratic. We demonstrate and assess the imputation
result for this fit in the following diagnostics graph.

Similarly, the quantiles between 0.015 and 0.095 of imputation residual is plotted 
against to the corresponding quantiles from a unit Normal distribution in Figure
\href{../plots/a1950/spaimpute/elev/d2/span0.015/a1950.spaResidcenter.bytime.pdf}
{\ref*{elev2.0.015.Residcenter.bytime}}. As we can notice, even though the 
distribution of residual still has a longer tail than unit Normal distribution,
the range of it for each month is about from -2 to 2 collectively, which is about 
60 percent less of the range of residual in the previous imputation model. Also
the longer tail issue becomes less severe than previous one.

\begin{framed}
\begin{center}
  \href{../plots/a1950/spaimpute/elev/d2/span0.015/a1950.spaResidcenter.bytime.pdf}
  {Link to figure}
  \captionof{figure}
  {The normal quantiles between 0.015 and 0.985 of residual of spatial imputation}
  \label{elev2.0.015.Residcenter.bytime}
\end{center}
\end{framed}

Next, we generate the diagnostics plots about the residual against to one of the 
predictor variables to detect any pattern left in the residual with respect to
the predictors. As shown in Figure
\href{../plots/a1950/spaimpute/elev/d2/span0.015/a1950.spaResid.vs.lat.lon.pdf}
{\ref*{elev.0.015.Residvslat.lon}}, imputation residual is drawn against to 
latitude conditional on longitude. Again, longitude is equally cut into 20 
intervals, and a loess smoothing curve with span equal to 0.25 and degree 1 is
added, which is approximately a flat horizontal line around 0. Furthermore, compared 
with Figure 
\href{../plots/a1950/spaimpute/nonelev/span0.05/a1950.spaResid.vs.lat.lon.pdf}
{\ref*{nonelev.0.05.Residvslat.lon}}, which does not include elevation as a
conditional parameter, the outlier issue in the first five panels are not as 
severe as it in Figure 
\href{../plots/a1950/spaimpute/nonelev/span0.05/a1950.spaResid.vs.lat.lon.pdf}
{\ref*{nonelev.0.05.Residvslat.lon}}, or even disappeared in some of months.
Besides that, the station \texttt{275639} with elevation 1909 meters, longitude 
-71.30 degree and altitude 44.27 degree in the last panel is not an outlier anymore.

\begin{framed}
\begin{center}
  \href{../plots/a1950/spaimpute/elev/d2/span0.015/a1950.spaResid.vs.lat.lon.pdf}
  {Link to figure}
  \captionof{figure}
  {The residual of spatial imputation against to latitude conditional on longitude}
  \label{elev.0.015.Residvslat.lon}
\end{center}
\end{framed}

Then, We switch longitude and latitude, and plot the residuals against to longitude
conditional on 20 intervals of latitude. Still most of the outliers existed in 
the previous imputation in section~\ref{sec:impute.w/o.elev} are vanished across
all panels in Figure
\href{../plots/a1950/spaimpute/elev/d2/span0.015/a1950.spaResid.vs.lon.lat.pdf}
{\ref*{elev.0.015.Residvslon.lat}}. Same for the outliers in the region of 
Appalachian Mountains. Consequently, we keep the result of current fitting, span 
equal to 0.015 and degree equal to 2, to only impute the missing values in the 
original dataset.

\begin{framed}
\begin{center}
  \href{../plots/a1950/spaimpute/elev/d2/span0.015/a1950.spaResid.vs.lon.lat.pdf}
  {Link to figure}
  \captionof{figure}
  {The residual of spatial imputation against to longitude conditional on latitude}
  \label{elev.0.015.Residvslon.lat}
\end{center}
\end{framed}

Of course, we are not trying to perfectly capture the spatial variation in the
maximum temperature within the imputation. It is only to fill in the missing 
values and solve the consecutive missing value issue, which will be a rigorous 
problem in \texttt{stlplus} fit in the next section. Another crucial reason for
imputing spatially is for the prediction at a new location. Suppose we would like
to carry out prediction at a new location where there is no historical data at
all. So all information we can borrow for predicting is based on spatial dimension 
instead of time dimension. We impute by assuming there is only very limited 
relation between different locations in two different months. After the imputation
at the new location for each month in parallel, we then will carry out the fitting
in time dimension as demonstrated in the following section since we generated an 
estimated time series at the new location.

